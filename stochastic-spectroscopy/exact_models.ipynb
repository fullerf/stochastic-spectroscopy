{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/fdfuller/work/dgp_iwvi_gpflow2/')\n",
    "import gpflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Tuple, Optional, List, Union\n",
    "import dgp_iwvi_gpflow2.layers as layers\n",
    "from dgp_iwvi_gpflow2.reference_spectra import *\n",
    "import attr\n",
    "import tensorflow_probability as tfp\n",
    "from matplotlib.pyplot import *\n",
    "from gpflow.utilities import print_summary\n",
    "import h5py\n",
    "from sklearn.neighbors import KernelDensity\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['GPMM1D_Exact', 'GPMM2D_Exact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegressionData = Tuple[tf.Tensor, tf.Tensor]\n",
    "AuxRegressionData = Tuple[tf.Tensor, tf.Tensor, tf.Tensor]\n",
    "InputData = tf.Tensor\n",
    "MeanVarKL = Tuple[tf.Tensor, tf.Tensor, tf.Tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPMM1D_Exact(gpflow.models.BayesianModel, gpflow.models.InternalDataTrainingLossMixin):\n",
    "    r\"\"\"\n",
    "    Gaussian Process Regression with GPMM Likelihood, implemented in a way to exploit linear \n",
    "    identities that enable efficient evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Tuple[tf.Tensor, tf.Tensor],\n",
    "        kernel: gpflow.kernels.Kernel,\n",
    "        mean_function: Optional[gpflow.mean_functions.MeanFunction] = None,\n",
    "        noise_variance: float = 1.0,\n",
    "        jitter: float = 1e-8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.likelihood = gpflow.likelihoods.Gaussian(noise_variance)\n",
    "        self.kernel = kernel\n",
    "        self._pred_jitter_kernel = gpflow.kernels.White(variance=jitter) # needed in prediction only\n",
    "        gpflow.set_trainable(self._pred_jitter_kernel.variance, False)\n",
    "        W, y = data\n",
    "        R = W.shape[-1]\n",
    "        raxis = np.linspace(-1.0, 1.0, R)[:,None]\n",
    "        self.raxis = tf.convert_to_tensor(raxis, dtype=gpflow.default_float())\n",
    "        self.data = tuple([tf.convert_to_tensor(d, dtype=gpflow.config.default_float()) for d in data])\n",
    "        # reassign for convenience now that we're in tensorflow mode\n",
    "        W, Y = self.data\n",
    "        self._WTW = tf.matmul(W,W, transpose_a=True)\n",
    "        self._WTy = tf.matmul(W,y, transpose_a=True)\n",
    "        self._yTy = tf.matmul(y,y, transpose_a=True)\n",
    "        self.N = tf.cast(tf.shape(W)[0], dtype=gpflow.default_float())\n",
    "        if mean_function is None:\n",
    "            self.prior_mean_func = gpflow.mean_functions.Zero()\n",
    "            self.zpm = True\n",
    "        else:\n",
    "            self.prior_mean_func = mean_function\n",
    "            self.zpm = False\n",
    "        self.jitter = tf.cast(jitter, dtype=gpflow.default_float())\n",
    "        self.log2pi = tf.cast(np.log(np.pi*2), dtype=gpflow.default_float())\n",
    "        \n",
    "    @tf.function\n",
    "    def maximum_log_likelihood_objective(self) -> tf.Tensor:\n",
    "        return self.log_marginal_likelihood()\n",
    "\n",
    "    def log_marginal_likelihood(self) -> tf.Tensor:\n",
    "        r\"\"\"\n",
    "        Computes the log marginal likelihood.\n",
    "        .. math::\n",
    "            \\log p(Y | \\theta).\n",
    "        \"\"\"\n",
    "        R = tf.shape(self.raxis)[0]\n",
    "        prior_mean = self.prior_mean_func(self.raxis)\n",
    "        Krr = self.kernel(self.raxis, full_cov=True)\n",
    "        Kdiag = tf.linalg.diag_part(Krr)\n",
    "        jitter_vec = tf.fill([tf.shape(Kdiag)[-1]], self.jitter)\n",
    "        K = tf.linalg.set_diag(Krr, Kdiag + jitter_vec)\n",
    "        L = tf.linalg.cholesky(K)\n",
    "        inv_var = 1/self.likelihood.variance\n",
    "        inv_var_squared = tf.square(inv_var)\n",
    "        sigma = tf.sqrt(self.likelihood.variance)\n",
    "        B = tf.linalg.eye(R, dtype=gpflow.default_float()) + inv_var*tf.matmul(tf.matmul(L, self._WTW, transpose_a=True),L)\n",
    "        LB = tf.linalg.cholesky(B)\n",
    "        A = tf.linalg.triangular_solve(LB, tf.matmul(L, self._WTW, transpose_a=True), lower=True, adjoint=False)\n",
    "        yB = tf.linalg.triangular_solve(LB, tf.matmul(L, self._WTy, transpose_a=True), lower=True, adjoint=False)\n",
    "        if not self.zpm:\n",
    "            t1a = -0.5*inv_var*( self._yTy - \n",
    "                                    2.0*tf.matmul(prior_mean, self._WTy, transpose_a=True) + \n",
    "                                    tf.matmul(tf.matmul(prior_mean, self._WTW, transpose_a=True), prior_mean)\n",
    "                               )\n",
    "            Am = tf.matmul(A, prior_mean)\n",
    "            t1b = 0.5*inv_var_squared*( tf.matmul(yB, yB, transpose_a=True) - \n",
    "                                       2.0*tf.matmul(Am, yB, transpose_a=True) + \n",
    "                                       tf.matmul(Am, Am, transpose_a=True) )\n",
    "        else:\n",
    "            t1a = -0.5*inv_var*( self._yTy)\n",
    "            t1b = 0.5*inv_var_squared*(tf.matmul(yB, yB, transpose_a=True))\n",
    "        t1 = tf.reduce_sum(t1a + t1b)\n",
    "        t2 = self.N*tf.math.log(sigma) + tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LB)))\n",
    "#         t3 = 0.5*self.N*self.log2pi # constant wrt to parameters, so can be dropped.\n",
    "        return (t1 - t2)# + t3\n",
    "    def predict_f(self, raxis: Optional[tf.Tensor] = None,\n",
    "                        full_cov: bool = False) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        r\"\"\"\n",
    "        This method computes predictions at the requested axes if supplied or at the training axes if not\n",
    "        \"\"\"\n",
    "        if raxis is None:\n",
    "            raxis = self.raxis\n",
    "        R = tf.shape(raxis)[0]\n",
    "        prior_mean = self.prior_mean_func(raxis)\n",
    "        prior_mean_train = self.prior_mean_func(self.raxis)\n",
    "        inv_var = 1/self.likelihood.variance\n",
    "        inv_var_squared = tf.square(inv_var)\n",
    "        log_var = tf.math.log(self.likelihood.variance)\n",
    "        # kernels needed for prediction to potentially new axes\n",
    "        # read Ksfr as reduced_kernel(prediction_points, training_points)\n",
    "        # read Ksfq as observed_kernel(prediction_points, training_points)\n",
    "        Ksfr = self.kernel(raxis, self.raxis, full_cov=True) + \\\n",
    "                self._pred_jitter_kernel(raxis, self.raxis, full_cov=True)\n",
    "        Kssr = self.kernel(raxis, full_cov=full_cov)\n",
    "        # kernel bits from training\n",
    "        Krr = self.kernel(self.raxis, full_cov=True)\n",
    "#         Krr += self.jitter * tf.eye(tf.shape(Krr)[-1], dtype=gpflow.config.default_float())\n",
    "#         Kqq += self.jitter * tf.eye(tf.shape(Kqq)[-1], dtype=gpflow.config.default_float())\n",
    "#         Lr = tf.linalg.cholesky(Krr)\n",
    "#         Lq = tf.linalg.cholesky(Kqq)\n",
    "        Krr_diag = tf.linalg.diag_part(Krr)\n",
    "        jitter_vec_r = tf.fill([tf.shape(Krr_diag)[-1]], self.jitter)\n",
    "        Krr_full_rank = tf.linalg.set_diag(Krr, Krr_diag + jitter_vec_r)\n",
    "        Lr = tf.linalg.cholesky(Krr_full_rank)\n",
    "\n",
    "        B0 = inv_var * tf.matmul(tf.matmul(Lr, self._WTW, transpose_a=True), Lr)\n",
    "        B0_diag = tf.linalg.diag_part(B0)\n",
    "        ones_r = tf.fill([tf.shape(B0_diag)[-1]], tf.convert_to_tensor(1., dtype=gpflow.default_float()))\n",
    "        B = tf.linalg.set_diag(B0, B0_diag + ones_r)\n",
    "        Lb = tf.linalg.cholesky(B)\n",
    "        yb = tf.linalg.triangular_solve(Lb, tf.matmul(Lr, self._WTy, transpose_a=True), lower=True, adjoint=False)\n",
    "        Lbinvt_yb = tf.linalg.triangular_solve(Lb, yb, lower=True, adjoint=True)        \n",
    "        tm1 = inv_var * tf.matmul(Ksfr, self._WTy)\n",
    "        tm2 = inv_var_squared * tf.matmul(Ksfr, tf.matmul(self._WTW, tf.matmul(Lr, Lbinvt_yb)))\n",
    "        tm3 = prior_mean - inv_var*tf.matmul(Ksfr, tf.matmul(self._WTW, prior_mean_train))\n",
    "        mb = tf.linalg.triangular_solve(Lb, tf.matmul(tf.matmul(Lr, self._WTW, transpose_a=True), prior_mean_train), \n",
    "                                        lower=True, adjoint=False)\n",
    "        Lbinvt_mb = tf.linalg.triangular_solve(Lb, mb, lower=True, adjoint=True)\n",
    "        tm4 = inv_var_squared * tf.matmul(Ksfr, tf.matmul(self._WTW, tf.matmul(Lr, Lbinvt_mb)))\n",
    "        m = tm1 - tm2 + tm3 + tm4\n",
    "        A = tf.linalg.triangular_solve(Lb, tf.matmul(Lr, self._WTW, transpose_a=True), lower=True, adjoint=False)\n",
    "        V = tf.matmul(A, Ksfr, transpose_b=True)\n",
    "        if not full_cov:\n",
    "            v = Kssr - \\\n",
    "                inv_var*tf.linalg.diag_part(tf.matmul(Ksfr, tf.matmul(self._WTW, Ksfr, transpose_b=True))) + \\\n",
    "                inv_var_squared*tf.linalg.diag_part(tf.matmul(V, V, transpose_a=True))\n",
    "        else:\n",
    "            v = Kssr - \\\n",
    "                inv_var*tf.matmul(Ksfr, tf.matmul(self._WTW, Ksfr, transpose_b=True)) + \\\n",
    "                inv_var_squared*tf.matmul(V, V, transpose_a=True)\n",
    "        return m, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_vec_identity(A: tf.Tensor,B: tf.Tensor,C: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    For matrices, A, B, and C (i.e. 2D tensors):\n",
    "    \n",
    "    implements kron(A,B) @ C, where C is interpreted as a \"batched vector\" with\n",
    "    the batch dim being the columns. C must have rows = A.shape[-1]*B.shape[-1], \n",
    "    but can have any number of columns.\n",
    "    \n",
    "    The normal vec identity vec(is B @ vec^-1(C) @ A.T) = kron(A,B) @ C\n",
    "    for row major vec identity it is vec(is A @ vec^-1(C) @ B.T) = kron(A,B) @ C\n",
    "    as tf is row major, we use the latter.\n",
    "    \n",
    "    to batch this, we reshape C like: (C.T).reshape(C.shape[-1], A.shape[-1], B.shape[-1])\n",
    "    The matrix multiplication is carried out with broadcasting via einsum:\n",
    "    \n",
    "    result = tf.einsum('...ij,zjk,...lk->ilz').reshape(A.shape[0]*B.shape[0],C.shape[-1])\n",
    "    \"\"\"\n",
    "    L = tf.shape(B)[0]\n",
    "    K = tf.shape(B)[1]\n",
    "    I = tf.shape(A)[0]\n",
    "    J = tf.shape(A)[1]\n",
    "    Z = tf.shape(C)[1]\n",
    "    Cr = tf.reshape(tf.transpose(C), [Z, J, K])\n",
    "    A = tf.tile(A[None,:,:], [Z,1,1])\n",
    "    B = tf.tile(B[None,:,:], [Z,1,1])\n",
    "    return tf.reshape(tf.einsum('aij,ajk,alk->ila',A,Cr,B), [I*L,Z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kron(op1: tf.Tensor, op2: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    This version uses the LinearOperatorKronecker function built into TensorFlow\n",
    "    \"\"\"\n",
    "    lop1 = tf.linalg.LinearOperatorFullMatrix(op1)\n",
    "    lop2 = tf.linalg.LinearOperatorFullMatrix(op2)\n",
    "    return tf.linalg.LinearOperatorKronecker([lop1,lop2]).to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_prod(*ops):\n",
    "    mops = tf.meshgrid(*ops,indexing='ij')\n",
    "    rmops = [tf.reshape(op,[-1,1]) for op in mops]\n",
    "    return tf.concat(rmops,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-245ea225c084>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGPMM2D_Exact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBayesianModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInternalDataTrainingLossMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     r\"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mGaussian\u001b[0m \u001b[0mProcess\u001b[0m \u001b[0mRegression\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mGPMM\u001b[0m \u001b[0mLikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mRIXS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gpflow' is not defined"
     ]
    }
   ],
   "source": [
    "class GPMM2D_Exact(gpflow.models.BayesianModel, gpflow.models.InternalDataTrainingLossMixin):\n",
    "    r\"\"\"\n",
    "    Gaussian Process Regression with GPMM Likelihood, for RIXS (2D)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Tuple[tf.Tensor, tf.Tensor],\n",
    "        reduced_kernel: gpflow.kernels.Kernel,\n",
    "        observed_kernel: gpflow.kernels.Kernel,\n",
    "        mean_function: Optional[gpflow.mean_functions.MeanFunction] = None,\n",
    "        noise_variance: float = 1.0,\n",
    "        jitter: float = 1e-8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.likelihood = gpflow.likelihoods.Gaussian(noise_variance)\n",
    "        self.reduced_kernel = reduced_kernel # this will be the kernel for absorption axis\n",
    "        self.observed_kernel = observed_kernel # this is the kernel for fluorescence axis\n",
    "        self._pred_jitter_kernel = gpflow.kernels.White(variance=jitter) # needed in prediction only\n",
    "        gpflow.set_trainable(self._pred_jitter_kernel.variance, False)\n",
    "        W, Y = data\n",
    "        R = W.shape[-1]  # the reduced axis length\n",
    "        Q = Y.shape[-1]  # the observed axis length\n",
    "        raxis = np.linspace(-1.0, 1.0, R)[:,None]  # reduced axis\n",
    "        qaxis = np.linspace(-1.0, 1.0, Q)[:,None]  # observed axis\n",
    "        self.raxis = tf.convert_to_tensor(raxis, dtype=gpflow.default_float())\n",
    "        self.qaxis = tf.convert_to_tensor(qaxis, dtype=gpflow.default_float())\n",
    "        # full_axis = 2D axis ordered to match kron(reduced_axis, observed_axis)\n",
    "        self.full_axis = cartesian_prod(self.raxis, self.qaxis)  \n",
    "        self.data = tuple([tf.convert_to_tensor(d, dtype=gpflow.config.default_float()) for d in data])\n",
    "        # reassign for convenience now that we're in tensorflow mode\n",
    "        W, Y = self.data\n",
    "        # we need to store W for prediction, but not training\n",
    "        self._W = W\n",
    "        # make some cached quantities\n",
    "        self._WTW = tf.matmul(W,W, transpose_a=True)\n",
    "        self._YTW = tf.matmul(Y,W, transpose_a=True)\n",
    "        self._yTy = tf.reduce_sum(Y*Y) # equivalent to fvec(Y).T @ fvec(Y)\n",
    "        self._N = tf.cast(tf.shape(W)[0], dtype=gpflow.default_float())\n",
    "        self._R = tf.cast(tf.shape(W)[-1], dtype=gpflow.default_float())\n",
    "        self._Q = tf.cast(tf.shape(Y)[-1], dtype=gpflow.default_float())\n",
    "        if mean_function is None:\n",
    "            self.prior_mean_func = gpflow.mean_functions.Zero()\n",
    "            self.zpm = True\n",
    "        else:\n",
    "            self.prior_mean_func = mean_function\n",
    "            self.zpm = False\n",
    "        self.jitter = tf.cast(jitter, dtype=gpflow.default_float())\n",
    "        self.log2pi = tf.cast(np.log(np.pi*2), dtype=gpflow.default_float())\n",
    "        \n",
    "    @tf.function\n",
    "    def maximum_log_likelihood_objective(self) -> tf.Tensor:\n",
    "        return self.log_marginal_likelihood()\n",
    "\n",
    "    def log_marginal_likelihood(self) -> tf.Tensor:\n",
    "        r\"\"\"\n",
    "        Computes the log marginal likelihood.\n",
    "        .. math::\n",
    "            \\log p(Y | \\theta).\n",
    "        \"\"\"\n",
    "        R = tf.shape(self.raxis)[0]\n",
    "        Q = tf.shape(self.qaxis)[0]\n",
    "        prior_mean = self.prior_mean_func(self.full_axis)\n",
    "        inv_var = 1/self.likelihood.variance\n",
    "        inv_var_squared = tf.square(inv_var)\n",
    "        log_var = tf.math.log(self.likelihood.variance)\n",
    "        Krr = self.reduced_kernel(self.raxis, full_cov=True)\n",
    "        Kqq = self.observed_kernel(self.qaxis, full_cov=True)\n",
    "#         Krr += self.jitter * tf.eye(tf.shape(Krr)[-1], dtype=gpflow.config.default_float())\n",
    "#         Kqq += self.jitter * tf.eye(tf.shape(Kqq)[-1], dtype=gpflow.config.default_float())\n",
    "#         Lr = tf.linalg.cholesky(Krr)\n",
    "#         Lq = tf.linalg.cholesky(Kqq)\n",
    "        Krr_diag = tf.linalg.diag_part(Krr)\n",
    "        Kqq_diag = tf.linalg.diag_part(Kqq)\n",
    "        jitter_vec_r = tf.fill([tf.shape(Krr_diag)[-1]], self.jitter)\n",
    "        jitter_vec_q = tf.fill([tf.shape(Kqq_diag)[-1]], self.jitter)\n",
    "        Krr_full_rank = tf.linalg.set_diag(Krr, Krr_diag + jitter_vec_r)\n",
    "        Kqq_full_rank = tf.linalg.set_diag(Kqq, Kqq_diag + jitter_vec_q)\n",
    "        Lr = tf.linalg.cholesky(Krr_full_rank)\n",
    "        Lq = tf.linalg.cholesky(Kqq_full_rank)\n",
    "\n",
    "        \n",
    "        B0 = kron(inv_var * tf.matmul(tf.matmul(Lr, self._WTW, transpose_a=True), Lr),\n",
    "                  tf.matmul(Lq, Lq, transpose_a=True))\n",
    "        B0_diag = tf.linalg.diag_part(B0)\n",
    "        ones_rq = tf.fill([tf.shape(B0_diag)[-1]], tf.convert_to_tensor(1., dtype=gpflow.default_float()))\n",
    "        B = tf.linalg.set_diag(B0, B0_diag + ones_rq)\n",
    "        Lb = tf.linalg.cholesky(B)\n",
    "        \n",
    "        A = tf.linalg.triangular_solve(Lb, kron(tf.matmul(Lr, self._WTW, transpose_a=True),\n",
    "                                                tf.transpose(Lq)),lower=True, adjoint=False)\n",
    "        \n",
    "        LtWty = tf.reshape(tf.transpose(tf.matmul(tf.matmul(Lq, self._YTW, transpose_a=True), Lr)),[-1,1])\n",
    "        vecYtW = tf.reshape(tf.transpose(self._YTW),[-1,1])\n",
    "        Wm = batched_vec_identity(self._WTW, tf.linalg.eye(Q, dtype=gpflow.config.default_float()),prior_mean)\n",
    "        yb = tf.linalg.triangular_solve(Lb, LtWty, lower=True, adjoint=False)\n",
    "        t1b1 = tf.reduce_sum(yb*yb)\n",
    "        if self.zpm:\n",
    "            t1a = 0.5*inv_var*(self._yTy)\n",
    "            t1b = -0.5*inv_var_squared*(t1b1)\n",
    "        else:\n",
    "            Am = tf.matmul(A, prior_mean)\n",
    "            t1a1 = 2.0*tf.matmul(prior_mean, vecYtW, transpose_a=True)\n",
    "            t1a2 = tf.matmul(prior_mean, Wm, transpose_a=True)\n",
    "            t1a = 0.5*inv_var*(self._yTy - t1a1 + t1a2)\n",
    "            t1b2 = 2.0*tf.matmul(Am, yb, transpose_a=True)\n",
    "            t1b3 = tf.reduce_sum(Am*Am)\n",
    "            t1b = -0.5*inv_var_squared*(t1b1 - t1b2 + t1b3)\n",
    "        t1 = t1a + t1b\n",
    "        t2 = 0.5 * self._N * self._Q * log_var + tf.reduce_sum(tf.math.log(tf.linalg.diag_part(Lb)))\n",
    "        t3 = 0.5 * self._N * self._Q * self.log2pi\n",
    "        loss = -t1 - t2# -t3 # t3 is constant wrt parameters, removing for numerical stability of gradients\n",
    "        return tf.reduce_sum(loss)\n",
    "\n",
    "    def predict_f(self, raxis: Optional[tf.Tensor] = None,\n",
    "                        qaxis: Optional[tf.Tensor] = None,\n",
    "                        full_cov: bool = False) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        r\"\"\"\n",
    "        This method computes predictions at the requested axes if supplied or at the training axes if not\n",
    "        \"\"\"\n",
    "        if raxis is None:\n",
    "            raxis = self.raxis\n",
    "        if qaxis is None:\n",
    "            qaxis = self.qaxis\n",
    "        full_axis = cartesian_prod(raxis, qaxis)\n",
    "        R = tf.shape(raxis)[0]\n",
    "        Q = tf.shape(qaxis)[0]\n",
    "        prior_mean = self.prior_mean_func(full_axis)\n",
    "        prior_mean_train = self.prior_mean_func(cartesian_prod(self.raxis, self.qaxis))\n",
    "        inv_var = 1/self.likelihood.variance\n",
    "        inv_var_squared = tf.square(inv_var)\n",
    "        log_var = tf.math.log(self.likelihood.variance)\n",
    "        # kernels needed for prediction to potentially new axes\n",
    "        # read Ksfr as reduced_kernel(prediction_points, training_points)\n",
    "        # read Ksfq as observed_kernel(prediction_points, training_points)\n",
    "        Ksfr = self.reduced_kernel(raxis, self.raxis, full_cov=True) + \\\n",
    "                self._pred_jitter_kernel(raxis, self.raxis, full_cov=True)\n",
    "        Kssr = self.reduced_kernel(raxis, full_cov=full_cov)  # diagonal\n",
    "        Ksfq = self.observed_kernel(qaxis, self.qaxis, full_cov=True) + \\\n",
    "                self._pred_jitter_kernel(qaxis, self.qaxis, full_cov=True)\n",
    "        Kssq = self.observed_kernel(qaxis, full_cov=full_cov)  # diagonal\n",
    "        # kernel bits from training\n",
    "        Krr = self.reduced_kernel(self.raxis, full_cov=True)\n",
    "        Kqq = self.observed_kernel(self.qaxis, full_cov=True)\n",
    "#         Krr += self.jitter * tf.eye(tf.shape(Krr)[-1], dtype=gpflow.config.default_float())\n",
    "#         Kqq += self.jitter * tf.eye(tf.shape(Kqq)[-1], dtype=gpflow.config.default_float())\n",
    "#         Lr = tf.linalg.cholesky(Krr)\n",
    "#         Lq = tf.linalg.cholesky(Kqq)\n",
    "        Krr_diag = tf.linalg.diag_part(Krr)\n",
    "        Kqq_diag = tf.linalg.diag_part(Kqq)\n",
    "        jitter_vec_r = tf.fill([tf.shape(Krr_diag)[-1]], self.jitter)\n",
    "        jitter_vec_q = tf.fill([tf.shape(Kqq_diag)[-1]], self.jitter)\n",
    "        Krr_full_rank = tf.linalg.set_diag(Krr, Krr_diag + jitter_vec_r)\n",
    "        Kqq_full_rank = tf.linalg.set_diag(Kqq, Kqq_diag + jitter_vec_q)\n",
    "        Lr = tf.linalg.cholesky(Krr_full_rank)\n",
    "        Lq = tf.linalg.cholesky(Kqq_full_rank)\n",
    "\n",
    "        B0 = kron(inv_var * tf.matmul(tf.matmul(Lr, self._WTW, transpose_a=True), Lr),\n",
    "                  tf.matmul(Lq, Lq, transpose_a=True))\n",
    "        B0_diag = tf.linalg.diag_part(B0)\n",
    "        ones_rq = tf.fill([tf.shape(B0_diag)[-1]], tf.convert_to_tensor(1., dtype=gpflow.default_float()))\n",
    "        B = tf.linalg.set_diag(B0, B0_diag + ones_rq)\n",
    "        Lb = tf.linalg.cholesky(B)\n",
    "        LtWty = tf.reshape(tf.transpose(tf.matmul(tf.matmul(Lq, self._YTW, transpose_a=True), Lr)),[-1,1])\n",
    "        vecYtW = tf.reshape(tf.transpose(self._YTW),[-1,1])\n",
    "        yb = tf.linalg.triangular_solve(Lb, LtWty, lower=True, adjoint=False)\n",
    "        Lbinvt_yb = tf.linalg.triangular_solve(Lb, yb, lower=True, adjoint=True)        \n",
    "        tm1 = inv_var * batched_vec_identity(Ksfr, Ksfq, vecYtW)\n",
    "        tm2 = inv_var_squared * batched_vec_identity(tf.matmul(tf.matmul(Ksfr, self._WTW), Lr),\n",
    "                                                    tf.matmul(Ksfq, Lq),\n",
    "                                                    Lbinvt_yb)\n",
    "        tm3 = prior_mean - inv_var * batched_vec_identity(tf.matmul(Ksfr, self._WTW), Ksfq, prior_mean_train)\n",
    "        mb = tf.linalg.triangular_solve(Lb, batched_vec_identity(tf.matmul(Lr, self._WTW, transpose_a=True),\n",
    "                                                          tf.transpose(Lq),\n",
    "                                                          prior_mean_train), lower=True, adjoint=False)\n",
    "        Lbinvt_mb = tf.linalg.triangular_solve(Lb, mb, lower=True, adjoint=True)\n",
    "        tm4 = inv_var_squared * batched_vec_identity(tf.matmul(tf.matmul(Ksfr, self._WTW), Lr),\n",
    "                                                    tf.matmul(Ksfq, Lq),\n",
    "                                                    Lbinvt_mb)\n",
    "        m = tm1 - tm2 + tm3 + tm4\n",
    "        D1 = kron(tf.transpose(tf.matmul(tf.matmul(Ksfr, self._WTW), Lr)),\n",
    "                  tf.transpose(tf.matmul(Ksfq, Lq)))\n",
    "        D = tf.linalg.triangular_solve(Lb, D1, lower=True, adjoint=False)\n",
    "        \n",
    "        if not full_cov:\n",
    "            v1a = tf.matmul(Ksfr, self._W, transpose_b=True)\n",
    "            v1 = tf.reduce_sum(v1a * v1a, -1)\n",
    "            v2 = tf.reduce_sum(Ksfq * Ksfq, -1)\n",
    "            va = kron(Kssr[:,None], Kssq[:,None])\n",
    "            vb = inv_var*kron(v1[:,None],v2[:,None])\n",
    "            vc = inv_var_squared*(tf.reduce_sum(D*D,-2))[:,None]\n",
    "            v = va - vb + vc\n",
    "    #         v = -vb + vc\n",
    "    #         v = (kron(Kssr[:,None], Kssq[:,None]) - \\\n",
    "    #              inv_var*kron(v1[:,None],v2[:,None]) + \\\n",
    "    #              inv_var_squared*(tf.reduce_sum(D*D,-2))[:,None])\n",
    "            m = tf.transpose(tf.reshape(m,[R,Q]))\n",
    "            v = tf.transpose(tf.reshape(v,[R,Q]))\n",
    "        else:\n",
    "            v1a = tf.matmul(Ksfr, self._W, transpose_b=True)\n",
    "            v1 = v1a @ tf.transpose(v1a)\n",
    "            v2 = Ksfq @ tf.transpose(Ksfq)\n",
    "            va = kron(Kssr, Kssq)\n",
    "            vb = inv_var*kron(v1,v2)\n",
    "            vc = inv_var_squared*(tf.transpose(D) @ D)\n",
    "            v = va - vb + vc\n",
    "    #         v = -vb + vc\n",
    "    #         v = (kron(Kssr[:,None], Kssq[:,None]) - \\\n",
    "    #              inv_var*kron(v1[:,None],v2[:,None]) + \\\n",
    "    #              inv_var_squared*(tf.reduce_sum(D*D,-2))[:,None])\n",
    "            m = tf.transpose(tf.reshape(m,[R,Q]))\n",
    "            v = tf.transpose(tf.reshape(v,[R*Q,R*Q]))\n",
    "        if Q == 1:\n",
    "            m = tf.squeeze(m)\n",
    "            v = tf.squeeze(v)\n",
    "        return m, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
