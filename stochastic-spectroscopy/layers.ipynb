{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/dgp_iwvi_gpflow2/')\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import gpflow\n",
    "import enum\n",
    "import collections\n",
    "from typing import Callable, Optional, Tuple, TypeVar, Union, List\n",
    "from gpflow.kernels import Kernel, MultioutputKernel\n",
    "from gpflow.mean_functions import MeanFunction, Zero\n",
    "from gpflow.inducing_variables import SeparateIndependentInducingVariables, SharedIndependentInducingVariables\n",
    "from gpflow.kullback_leiblers import gauss_kl as gauss_kl_gpflow\n",
    "import attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# avoiding use of defaults kwarg, to keep compatibility with Python3.6\n",
    "class RegularizerType(enum.Enum):\n",
    "    LOCAL = 0\n",
    "    GLOBAL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_kl(q_mu, q_sqrt, K=None):\n",
    "    \"\"\"\n",
    "    Wrapper for gauss_kl from gpflow that returns the negative log prob if q_sqrt is None. This can be  \n",
    "    for use in HMC: all that is required is to set q_sqrt to None and this function substitues the\n",
    "    negative log prob instead of the KL (so no need to set q_mu.prior = gpflow.priors.Gaussian(0, 1)). \n",
    "    Also, this allows the use of HMC in the unwhitened case. \n",
    "    \"\"\"\n",
    "    if q_sqrt is None:\n",
    "        # return negative log prob with q_mu as 'x', with mean 0 and cov K (or I, if None)\n",
    "        M, D = tf.shape(q_mu)[0], tf.shape(q_mu)[1]\n",
    "        I = tf.eye(M, dtype=q_mu.dtype)\n",
    "\n",
    "        if K is None:\n",
    "            L = I\n",
    "        else:\n",
    "            L = tf.cholesky(K + I * gpflow.default_jitter())\n",
    "\n",
    "        return -tf.reduce_sum(gpflow.logdensities.multivariate_normal(q_mu, tf.zeros_like(q_mu), L))\n",
    "\n",
    "    else:\n",
    "        # return kl\n",
    "        return gauss_kl_gpflow(q_mu, q_sqrt, K=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPLayer(gpflow.Module):\n",
    "    regularizer_type = RegularizerType.GLOBAL\n",
    "    def __init__(self, \n",
    "                 kernel: gpflow.kernels.Kernel,\n",
    "                 inducing: gpflow.inducing_variables.InducingVariables,\n",
    "                 mean_func: gpflow.mean_functions.MeanFunction,\n",
    "                 **kwargs\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        The range of supported options for sample conditional is not complete. The following do not\n",
    "        work:\n",
    "        \n",
    "        LinearCoregionalization: in order to get the proper behavior, you must have a separate kernel\n",
    "        for each independent GP. While some things evaluate with a single shared kernel, Kuu and Kuf\n",
    "        do not work properly and treat the number of latent gps as the number of separate kernels.\n",
    "        \n",
    "        LinearCoregionalization / SharedIndependentInducingVariable: In this case, you can only eval-\n",
    "        uate the propagation when full_cov = False. However, full_cov is possible if one uses\n",
    "        SeparateIndependentInducingVariable.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert issubclass(type(kernel), gpflow.kernels.Kernel)\n",
    "        assert issubclass(type(inducing), gpflow.inducing_variables.InducingVariables)\n",
    "        if not issubclass(type(kernel), gpflow.kernels.MultioutputKernel):\n",
    "            self.num_latent_gps = 1\n",
    "            self.output_dim = 1\n",
    "        else:\n",
    "            self.num_latent_gps = kernel.num_latent_gps\n",
    "            if not len(kernel.kernels) == self.num_latent_gps:\n",
    "                # we want to catch the error with LinearCoregionalization mentioned above\n",
    "                raise ValueError(\n",
    "                    f\"number of kernels should match number of latent gps \" \\\n",
    "                    f\"({self.num_latent_gps}) got {len(kernel.kernels)} kernels\"\n",
    "                )\n",
    "            if issubclass(type(kernel), gpflow.kernels.LinearCoregionalization):\n",
    "                self.output_dim = kernel.W.shape[-2]\n",
    "            else:\n",
    "                self.output_dim = self.num_latent_gps\n",
    "        self.kernel = kernel\n",
    "        self.inducing = inducing\n",
    "        if hasattr(self.inducing, 'inducing_variable_list'):\n",
    "            # case for separate independent\n",
    "                assert len(self.inducing.inducing_variable_list) == self.num_latent_gps, \\\n",
    "                            f\"Got {len(self.inducing.inducing_variable_list)} inducing variables, \" \\\n",
    "                            f\"but expected {self.num_latent_gps} gps from kernel. These should match.\"\n",
    "                self.in_features = self.inducing.inducing_variable_list[0].Z.shape[-1]\n",
    "                self.num_inducing = self.inducing.inducing_variable_list[0].Z.shape[-2]\n",
    "        elif hasattr(self.inducing, 'inducing_variable'):\n",
    "            # case for shared independent\n",
    "            self.in_features = self.inducing.inducing_variable.Z.shape[-1]\n",
    "            self.num_inducing = self.inducing.inducing_variable.Z.shape[-2]\n",
    "        else:\n",
    "            self.in_features = self.inducing.Z.shape[-1]\n",
    "            self.num_inducing = self.inducing.Z.shape[-2]\n",
    "        assert issubclass(type(mean_func), gpflow.mean_functions.MeanFunction)\n",
    "        self.mean = mean_func\n",
    "        if type(mean_func) is gpflow.mean_functions.Linear:\n",
    "            # more consistency checking\n",
    "            assert self.mean.A.shape[-1] == self.output_dim\n",
    "        \n",
    "        # Now for the storage of variational parameters\n",
    "        self.q_mu = gpflow.Parameter(np.zeros((self.num_inducing, self.num_latent_gps)), transform=None)\n",
    "        init_sqrt = np.tile(np.eye(self.num_inducing)[None, :, :], [self.num_latent_gps, 1, 1])\n",
    "        if 'scale_init_q_sqrt' in kwargs:\n",
    "            init_sqrt *= kwargs['scale_init_q_sqrt']\n",
    "        self.q_sqrt = gpflow.Parameter(init_sqrt, transform=gpflow.utilities.triangular())\n",
    "    \n",
    "    def propagate(self, F, num_samples=None, full_cov=False, **kwargs):\n",
    "        # In Hugh's code, he forces one to use full_cov = False for the case of a MoK. This has the effect\n",
    "        # that only the final layer uses full covariance (as his code uses a single output kernel for the\n",
    "        # final layer). This is inspite of the fact that he passes full_cov=True to all layers in the IWVI\n",
    "        # case. Since I don't want to hack GPFlow's conditional system, I will let the full_cov pass through\n",
    "        # and manually implement his behavior in the model.\n",
    "        samples, mean, cov = gpflow.conditionals.sample_conditional(F,\n",
    "                                                self.inducing,\n",
    "                                                self.kernel,\n",
    "                                                self.q_mu,\n",
    "                                                full_cov=full_cov,\n",
    "                                                q_sqrt=self.q_sqrt,\n",
    "                                                white=True,\n",
    "                                                num_samples=num_samples,\n",
    "                                               )\n",
    "        kl = gauss_kl(self.q_mu, self.q_sqrt)\n",
    "        mf = self.mean(F)\n",
    "        if num_samples is not None:\n",
    "            samples = samples + mf[...,None,:,:]\n",
    "        else:\n",
    "            samples = samples + mf\n",
    "        mean = mean + mf\n",
    "        return samples, mean, cov, kl\n",
    "    \n",
    "    def components(self, F, num_samples=None, full_cov=False, **kwargs):\n",
    "        # In Hugh's code, he forces one to use full_cov = False for the case of a MoK. This has the effect\n",
    "        # that only the final layer uses full covariance (as his code uses a single output kernel for the\n",
    "        # final layer). This is inspite of the fact that he passes full_cov=True to all layers in the IWVI\n",
    "        # case. Since I don't want to hack GPFlow's conditional system, I will let the full_cov pass through\n",
    "        # and manually implement his behavior in the model.\n",
    "        mean, cov = gpflow.conditionals.conditional(F,\n",
    "                                                self.inducing,\n",
    "                                                self.kernel,\n",
    "                                                self.q_mu,\n",
    "                                                full_cov=full_cov,\n",
    "                                                q_sqrt=self.q_sqrt,\n",
    "                                                white=True,\n",
    "                                               )\n",
    "        kl = gauss_kl(self.q_mu, self.q_sqrt)\n",
    "        mf = self.mean(F)\n",
    "        mean = mean + mf\n",
    "        return mean, cov, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(gpflow.Module):\n",
    "    def __init__(self, latent_dim: int,\n",
    "                 input_dim: int,\n",
    "                 network_dims: int,\n",
    "                 activation_func: Optional[Callable] = None):\n",
    "        \"\"\"\n",
    "        Encoder that uses GPflow params to encode the features.\n",
    "        Creates an MLP with input dimensions `input_dim` and produces\n",
    "        2 * `latent_dim` outputs.\n",
    "        :param latent_dim: dimension of the latent variable\n",
    "        :param input_dim: the MLP acts on data of `input_dim` dimensions\n",
    "        :param network_dims: dimensions of inner MLPs, e.g. [10, 20, 10]\n",
    "        :param activation_func: TensorFlow operation that can be used\n",
    "            as non-linearity between the layers (default: tanh).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.activation_func = activation_func or tf.nn.tanh\n",
    "\n",
    "        self.layer_dims = [input_dim, *network_dims, latent_dim * 2]\n",
    "\n",
    "        Ws, bs = [], []\n",
    "\n",
    "        for input_dim, output_dim in zip(self.layer_dims[:-1], self.layer_dims[1:]):\n",
    "            xavier_std = (2. / (input_dim + output_dim)) ** 0.5\n",
    "            W = np.random.randn(input_dim, output_dim) * xavier_std\n",
    "            Ws.append(gpflow.Parameter(W, dtype=gpflow.config.default_float()))\n",
    "            bs.append(gpflow.Parameter(np.zeros(output_dim), dtype=gpflow.config.default_float()))\n",
    "\n",
    "        self.Ws, self.bs = Ws, bs\n",
    "\n",
    "    def __call__(self, Z) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        o = tf.ones_like(Z)[..., :1, :1]  # for correct broadcasting\n",
    "        for i, (W, b, dim_in, dim_out) in enumerate(zip(self.Ws, self.bs, self.layer_dims[:-1], self.layer_dims[1:])):\n",
    "            Z0 = tf.identity(Z)\n",
    "            Z = tf.matmul(Z, o * W) + o * b\n",
    "\n",
    "            if i < len(self.bs) - 1:\n",
    "                Z = self.activation_func(Z)\n",
    "\n",
    "            if dim_out == dim_in:  # skip connection\n",
    "                Z += Z0\n",
    "\n",
    "        means, log_chol_diag = tf.split(Z, 2, axis=-1)\n",
    "        q_sqrt = tf.nn.softplus(log_chol_diag - 3.)  # bias it towards small vals at first\n",
    "        q_mu = means\n",
    "        return q_mu, q_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SASEEncoder(gpflow.Module):\n",
    "    def __init__(self, latent_dim: int,\n",
    "                 input_dim: int,\n",
    "                 network_dims: int,\n",
    "                 activation_func: Optional[Callable] = None):\n",
    "        \"\"\"\n",
    "        Encoder that uses GPflow params to encode the features.\n",
    "        Creates an MLP with input dimensions `input_dim` and produces\n",
    "        2 * `latent_dim` outputs. Unlike the standard encoder, this \n",
    "        expects an input of NR shape, and converts that to an output which is\n",
    "        (N*R)L, where L is the latent dim.\n",
    "        \n",
    "        :param latent_dim: dimension of the latent variable, i.e L\n",
    "        :param input_dim: the MLP acts on data of `input_dim` dimensions, i.e. R\n",
    "        :param network_dims: dimensions of inner MLPs, e.g. [10, 20, 10]\n",
    "        :param activation_func: TensorFlow operation that can be used\n",
    "            as non-linearity between the layers (default: tanh).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_dim = tf.convert_to_tensor([latent_dim], tf.int32)\n",
    "        self.activation_func = activation_func or tf.nn.tanh\n",
    "\n",
    "        self.layer_dims = [input_dim, *network_dims, input_dim * latent_dim * 2]\n",
    "\n",
    "        Ws, bs = [], []\n",
    "\n",
    "        for input_dim, output_dim in zip(self.layer_dims[:-1], self.layer_dims[1:]):\n",
    "            xavier_std = (2. / (input_dim + output_dim)) ** 0.5\n",
    "            W = np.random.randn(input_dim, output_dim) * xavier_std\n",
    "            Ws.append(gpflow.Parameter(W, dtype=gpflow.config.default_float()))\n",
    "            bs.append(gpflow.Parameter(np.zeros(output_dim), dtype=gpflow.config.default_float()))\n",
    "\n",
    "        self.Ws, self.bs = Ws, bs\n",
    "\n",
    "    def __call__(self, Z) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        N = tf.convert_to_tensor([tf.shape(Z)[-2]], dtype=tf.int32)\n",
    "        R = tf.convert_to_tensor([tf.shape(Z)[-1]], dtype=tf.int32)\n",
    "        batch_shape = tf.convert_to_tensor(tf.shape(Z)[:-2], dtype=tf.int32)\n",
    "        o = tf.ones_like(Z)[..., :1, :1]  # for correct broadcasting\n",
    "        for i, (W, b, dim_in, dim_out) in enumerate(zip(self.Ws, self.bs, self.layer_dims[:-1], self.layer_dims[1:])):\n",
    "            Z0 = tf.identity(Z)\n",
    "            Z = tf.matmul(Z, o * W) + o * b\n",
    "\n",
    "            if i < len(self.bs) - 1:\n",
    "                Z = self.activation_func(Z)\n",
    "\n",
    "            if dim_out == dim_in:  # skip connection\n",
    "                Z += Z0\n",
    "\n",
    "        means, log_chol_diag = tf.split(Z, 2, axis=-1)\n",
    "        q_sqrt = tf.nn.softplus(log_chol_diag - 3.)  # bias it towards small vals at first\n",
    "        q_mu = means #...N(L*R)\n",
    "        q_mu_reshaped = tf.reshape(q_mu, tf.concat([batch_shape, N*R, self.latent_dim],0))  # ...(N*R)L\n",
    "        q_sqrt_reshaped = tf.reshape(q_sqrt, tf.concat([batch_shape, N*R, self.latent_dim],0))\n",
    "        return q_mu_reshaped, q_sqrt_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingEncoder(gpflow.Module):\n",
    "    def __init__(self, latent_dim: int,\n",
    "                 nwords: int):\n",
    "        \"\"\"\n",
    "        Here we simply pass a shot index to an embedding lookup. This allows us to \n",
    "        randomly access a tensor more easily, but basically we're just storing latent\n",
    "        values for each shot.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_dim = int(latent_dim)\n",
    "        self.embedding = gpflow.Parameter(np.random.randn(nwords, 2*latent_dim),\n",
    "                                         dtype=gpflow.config.default_float())\n",
    "    @tf.function\n",
    "    def __call__(self, Z: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        Zenc = tf.nn.embedding_lookup(self.embedding, tf.squeeze(Z,-1))\n",
    "        means, log_chol_diag = tf.split(Zenc, 2, axis=-1)\n",
    "        q_sqrt = tf.nn.softplus(log_chol_diag - 3.)  # bias it towards small vals at first\n",
    "        q_mu = means\n",
    "        return q_mu, q_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmortizedLatentVariableLayer(gpflow.Module):\n",
    "    regularizer_type = RegularizerType.LOCAL\n",
    "    def __init__(self, latent_dim: int,\n",
    "                 XY_dim: Optional[int] = None,\n",
    "                 encoder: Optional[Callable] = None):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        if encoder is None:\n",
    "            assert XY_dim, 'must pass XY_dim or else an encoder'\n",
    "            encoder = Encoder(latent_dim, XY_dim, [20, 20])\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def propagate(self, F: tf.Tensor,\n",
    "                  inference_amortization_inputs: Optional[tf.Tensor] = None,\n",
    "                  is_sampled_local_regularizer: bool = False,\n",
    "                  **kwargs) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "        if inference_amortization_inputs is None:\n",
    "            \"\"\"\n",
    "            If there isn't an X and Y passed for the recognition model, this samples from the prior.\n",
    "            Optionally, q_mu and q_sqrt can be fed via a placeholder (e.g. for plotting purposes)\n",
    "            \"\"\"\n",
    "            shape = tf.concat([F.shape[:-1], tf.TensorShape([self.latent_dim])], 0)\n",
    "            q_mu = tf.zeros(shape, dtype=gpflow.config.default_float())\n",
    "            q_sqrt = tf.ones(shape, dtype=gpflow.config.default_float())\n",
    "        else:\n",
    "            q_mu, q_sqrt = self.encoder(inference_amortization_inputs)\n",
    "\n",
    "        # reparameterization trick to take a sample for W\n",
    "        eps = tf.random.normal(tf.shape(q_mu), dtype=gpflow.config.default_float())\n",
    "        W = q_mu + eps * q_sqrt\n",
    "\n",
    "        samples = tf.concat([F, W], -1)\n",
    "        mean = tf.concat([F, q_mu], -1)\n",
    "        cov = tf.concat([tf.zeros_like(F), q_sqrt ** 2], -1)\n",
    "\n",
    "        # the prior regularization\n",
    "        p = p = tfp.distributions.Normal(loc=tf.zeros(1,dtype=gpflow.config.default_float()),\n",
    "                             scale=tf.ones(1,dtype=gpflow.config.default_float()))\n",
    "        q = tfp.distributions.Normal(loc=q_mu, scale=q_sqrt)\n",
    "\n",
    "        if is_sampled_local_regularizer:\n",
    "            # for the IW models, we need to return a log q/p for each sample W\n",
    "            kl = q.log_prob(W) - p.log_prob(W)\n",
    "        else:\n",
    "            # for the VI models, we want E_q log q/p, which is closed form for Gaussians\n",
    "            kl = tfp.distributions.kl_divergence(q, p)\n",
    "\n",
    "        return samples, mean, cov, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmortizedSASELatentVariableLayer(gpflow.Module):\n",
    "    regularizer_type = RegularizerType.LOCAL\n",
    "    def __init__(self, latent_dim: int,\n",
    "                 sase_dim: int,\n",
    "                 encoder: Optional[Callable] = None):\n",
    "        super().__init__()\n",
    "        if encoder is None:\n",
    "            encoder = SASEEncoder(latent_dim, sase_dim, [50, 10, 50])\n",
    "        self.latent_dim = tf.convert_to_tensor([latent_dim], dtype=tf.int32)\n",
    "        self.sase_dim = tf.convert_to_tensor([sase_dim], dtype=tf.int32)\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def propagate(self, F: tf.Tensor,\n",
    "                  inference_amortization_inputs: Optional[tf.Tensor] = None,\n",
    "                  is_sampled_local_regularizer: bool = False,\n",
    "                  **kwargs) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "        if inference_amortization_inputs is None:\n",
    "            \"\"\"\n",
    "            If there isn't a SASE spec passed for the recognition model, this samples from the prior.\n",
    "            Optionally, q_mu and q_sqrt can be fed via a placeholder (e.g. for plotting purposes)\n",
    "            \"\"\"\n",
    "            batch_shape = tf.convert_to_tensor(tf.shape(F)[:-2], dtype=tf.int32) # ...\n",
    "            N = tf.convert_to_tensor([tf.shape(F)[-2]], dtype=tf.int32)\n",
    "            shape = tf.concat([batch_shape, N, self.latent_dim], 0) # ...(N)L\n",
    "            q_mu = tf.zeros(shape, dtype=gpflow.config.default_float())\n",
    "            q_sqrt = tf.ones(shape, dtype=gpflow.config.default_float())\n",
    "        else:\n",
    "            q_mu, q_sqrt = self.encoder(inference_amortization_inputs)\n",
    "\n",
    "        # reparameterization trick to take a sample for W\n",
    "        eps = tf.random.normal(tf.shape(q_mu), dtype=gpflow.config.default_float())\n",
    "        W = q_mu + eps * q_sqrt\n",
    "\n",
    "        samples = tf.concat([F, W], -1)\n",
    "        mean = tf.concat([F, q_mu], -1)\n",
    "        cov = tf.concat([tf.zeros_like(F), q_sqrt ** 2], -1)\n",
    "\n",
    "        # the prior regularization\n",
    "        p = p = tfp.distributions.Normal(loc=tf.zeros(1,dtype=gpflow.config.default_float()),\n",
    "                             scale=tf.ones(1,dtype=gpflow.config.default_float()))\n",
    "        q = tfp.distributions.Normal(loc=q_mu, scale=q_sqrt)\n",
    "\n",
    "        if is_sampled_local_regularizer:\n",
    "            # for the IW models, we need to return a log q/p for each sample W\n",
    "            kl = q.log_prob(W) - p.log_prob(W)\n",
    "        else:\n",
    "            # for the VI models, we want E_q log q/p, which is closed form for Gaussians\n",
    "            kl = tfp.distributions.kl_divergence(q, p)\n",
    "\n",
    "        return samples, mean, cov, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmortizedEmbeddingLatentVariableLayer(gpflow.Module):\n",
    "    regularizer_type = RegularizerType.LOCAL\n",
    "    def __init__(self, latent_dim: int, nwords: int, nembed: int = 10, nhidden: int = 20):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = EmbeddingEncoder(latent_dim, nwords)\n",
    "\n",
    "    def propagate(self, F: tf.Tensor,\n",
    "                  inference_amortization_inputs: Optional[tf.Tensor] = None,\n",
    "                  is_sampled_local_regularizer: bool = False,\n",
    "                  **kwargs) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "        if inference_amortization_inputs is None:\n",
    "            \"\"\"\n",
    "            If there isn't an X and Y passed for the recognition model, this samples from the prior.\n",
    "            Optionally, q_mu and q_sqrt can be fed via a placeholder (e.g. for plotting purposes)\n",
    "            \"\"\"\n",
    "            shape = tf.concat([F.shape[:-1], tf.TensorShape([self.latent_dim])], 0)\n",
    "            q_mu = tf.zeros(shape, dtype=gpflow.config.default_float())\n",
    "            q_sqrt = tf.ones(shape, dtype=gpflow.config.default_float())\n",
    "        else:\n",
    "            q_mu, q_sqrt = self.encoder(inference_amortization_inputs)\n",
    "\n",
    "        # reparameterization trick to take a sample for W\n",
    "        eps = tf.random.normal(tf.shape(q_mu), dtype=gpflow.config.default_float())\n",
    "        W = q_mu + eps * q_sqrt\n",
    "\n",
    "        samples = tf.concat([F, W], -1)\n",
    "        mean = tf.concat([F, q_mu], -1)\n",
    "        cov = tf.concat([tf.zeros_like(F), q_sqrt ** 2], -1)\n",
    "\n",
    "        # the prior regularization\n",
    "        p = p = tfp.distributions.Normal(loc=tf.zeros(1,dtype=gpflow.config.default_float()),\n",
    "                             scale=tf.ones(1,dtype=gpflow.config.default_float()))\n",
    "        q = tfp.distributions.Normal(loc=q_mu, scale=q_sqrt)\n",
    "\n",
    "        if is_sampled_local_regularizer:\n",
    "            # for the IW models, we need to return a log q/p for each sample W\n",
    "            kl = q.log_prob(W) - p.log_prob(W)\n",
    "        else:\n",
    "            # for the VI models, we want E_q log q/p, which is closed form for Gaussians\n",
    "            kl = tfp.distributions.kl_divergence(q, p)\n",
    "\n",
    "        return samples, mean, cov, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SASEReducedEncoder(gpflow.Module):\n",
    "    def __init__(self, latent_dim: int,\n",
    "                 input_dim: int,\n",
    "                 network_dims: int,\n",
    "                 activation_func: Optional[Callable] = None):\n",
    "        \"\"\"\n",
    "        Encoder that uses GPflow params to encode the features.\n",
    "        Creates an MLP with input dimensions `input_dim` and produces\n",
    "        2 * `latent_dim` outputs. Unlike the standard encoder, this \n",
    "        expects an input of NR shape, and converts that to an output which is\n",
    "        (N*R)L, where L is the latent dim.\n",
    "        \n",
    "        :param latent_dim: dimension of the latent variable, i.e L\n",
    "        :param input_dim: the MLP acts on data of `input_dim` dimensions, i.e. R\n",
    "        :param network_dims: dimensions of inner MLPs, e.g. [10, 20, 10]\n",
    "        :param activation_func: TensorFlow operation that can be used\n",
    "            as non-linearity between the layers (default: tanh).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_dim = tf.convert_to_tensor([latent_dim], tf.int32)\n",
    "        self.activation_func = activation_func or tf.nn.tanh\n",
    "\n",
    "        self.layer_dims = [input_dim, *network_dims, latent_dim * 2]\n",
    "\n",
    "        Ws, bs = [], []\n",
    "\n",
    "        for input_dim, output_dim in zip(self.layer_dims[:-1], self.layer_dims[1:]):\n",
    "            xavier_std = (2. / (input_dim + output_dim)) ** 0.5\n",
    "            W = np.random.randn(input_dim, output_dim) * xavier_std\n",
    "            Ws.append(gpflow.Parameter(W, dtype=gpflow.config.default_float()))\n",
    "            bs.append(gpflow.Parameter(np.zeros(output_dim), dtype=gpflow.config.default_float()))\n",
    "\n",
    "        self.Ws, self.bs = Ws, bs\n",
    "\n",
    "    def __call__(self, Z) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        N = tf.convert_to_tensor([tf.shape(Z)[-2]], dtype=tf.int32)\n",
    "        R = tf.convert_to_tensor([tf.shape(Z)[-1]], dtype=tf.int32)\n",
    "        batch_shape = tf.convert_to_tensor(tf.shape(Z)[:-2], dtype=tf.int32)\n",
    "        o = tf.ones_like(Z)[..., :1, :1]  # for correct broadcasting\n",
    "        for i, (W, b, dim_in, dim_out) in enumerate(zip(self.Ws, self.bs, self.layer_dims[:-1], self.layer_dims[1:])):\n",
    "            Z0 = tf.identity(Z)\n",
    "            Z = tf.matmul(Z, o * W) + o * b\n",
    "\n",
    "            if i < len(self.bs) - 1:\n",
    "                Z = self.activation_func(Z)\n",
    "\n",
    "            if dim_out == dim_in:  # skip connection\n",
    "                Z += Z0\n",
    "\n",
    "        means, log_chol_diag = tf.split(Z, 2, axis=-1)\n",
    "        q_sqrt = tf.nn.softplus(log_chol_diag - 3.)  # bias it towards small vals at first\n",
    "        q_mu = means #...N(L*R)\n",
    "        return q_mu, q_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmortizedSASEReducedLatentVariableLayer(gpflow.Module):\n",
    "    regularizer_type = RegularizerType.LOCAL\n",
    "    def __init__(self, latent_dim: int,\n",
    "                 sase_dim: int,\n",
    "                 encoder: Optional[Callable] = None):\n",
    "        super().__init__()\n",
    "        if encoder is None:\n",
    "            encoder = SASEReducedEncoder(latent_dim, sase_dim, [50, 10, 10])\n",
    "        self.latent_dim = tf.convert_to_tensor([latent_dim], dtype=tf.int32)\n",
    "        self.sase_dim = tf.convert_to_tensor([sase_dim], dtype=tf.int32)\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def propagate(self, F: tf.Tensor,\n",
    "                  inference_amortization_inputs: Optional[tf.Tensor] = None,\n",
    "                  is_sampled_local_regularizer: bool = False,\n",
    "                  **kwargs) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "        if inference_amortization_inputs is None:\n",
    "            \"\"\"\n",
    "            If there isn't a SASE spec passed for the recognition model, this samples from the prior.\n",
    "            Optionally, q_mu and q_sqrt can be fed via a placeholder (e.g. for plotting purposes)\n",
    "            \"\"\"\n",
    "            batch_shape = tf.convert_to_tensor(tf.shape(F)[:-2], dtype=tf.int32) # ...\n",
    "            N = tf.convert_to_tensor([tf.shape(F)[-2]], dtype=tf.int32)\n",
    "            shape = tf.concat([batch_shape, N, self.latent_dim], 0) # ...(N)L\n",
    "            q_mu = tf.zeros(shape, dtype=gpflow.config.default_float())\n",
    "            q_sqrt = tf.ones(shape, dtype=gpflow.config.default_float())\n",
    "        else:\n",
    "            q_mu, q_sqrt = self.encoder(inference_amortization_inputs)\n",
    "\n",
    "        # reparameterization trick to take a sample for W\n",
    "        eps = tf.random.normal(tf.shape(q_mu), dtype=gpflow.config.default_float())\n",
    "        W = q_mu + eps * q_sqrt\n",
    "        samples = tf.concat([F, W], -1)\n",
    "        mean = tf.concat([F, q_mu], -1)\n",
    "        cov = tf.concat([tf.zeros_like(F), q_sqrt ** 2], -1)\n",
    "\n",
    "        # the prior regularization\n",
    "        p = p = tfp.distributions.Normal(loc=tf.zeros(1,dtype=gpflow.config.default_float()),\n",
    "                             scale=tf.ones(1,dtype=gpflow.config.default_float()))\n",
    "        q = tfp.distributions.Normal(loc=q_mu, scale=q_sqrt)\n",
    "\n",
    "        if is_sampled_local_regularizer:\n",
    "            # for the IW models, we need to return a log q/p for each sample W\n",
    "            kl = q.log_prob(W) - p.log_prob(W)\n",
    "        else:\n",
    "            # for the VI models, we want E_q log q/p, which is closed form for Gaussians\n",
    "            kl = tfp.distributions.kl_divergence(q, p)\n",
    "\n",
    "        return samples, mean, cov, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmortizedLatentVariableLayer2(gpflow.Module):\n",
    "    regularizer_type = RegularizerType.LOCAL\n",
    "    def __init__(self, latent_dim: int,\n",
    "                 sase_dim: int,\n",
    "                 encoder_dims: Optional[List[int]] = None):\n",
    "        super().__init__()\n",
    "        if encoder_dims is None:\n",
    "            encoder = Encoder(latent_dim, sase_dim, [50, 10, 10])\n",
    "        else:\n",
    "            encoder = Encoder(latent_dim, sase_dim, encoder_dims)\n",
    "        self.latent_dim = tf.convert_to_tensor([latent_dim], dtype=tf.int32)\n",
    "        self.sase_dim = tf.convert_to_tensor([sase_dim], dtype=tf.int32)\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def propagate(self, F: tf.Tensor,\n",
    "                  inference_amortization_inputs: Optional[tf.Tensor] = None,\n",
    "                  is_sampled_local_regularizer: bool = False,\n",
    "                  **kwargs) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "        if inference_amortization_inputs is None:\n",
    "            \"\"\"\n",
    "            If there isn't a SASE spec passed for the recognition model, this samples from the prior.\n",
    "            Optionally, q_mu and q_sqrt can be fed via a placeholder (e.g. for plotting purposes)\n",
    "            \"\"\"\n",
    "            batch_shape = tf.convert_to_tensor(tf.shape(F)[:-2], dtype=tf.int32) # ...\n",
    "            N = tf.convert_to_tensor([tf.shape(F)[-2]], dtype=tf.int32)\n",
    "            shape = tf.concat([batch_shape, N, self.latent_dim], 0) # ...(N)L\n",
    "            q_mu = tf.zeros(shape, dtype=gpflow.config.default_float())\n",
    "            q_sqrt = tf.ones(shape, dtype=gpflow.config.default_float())\n",
    "        else:\n",
    "            q_mu, q_sqrt = self.encoder(inference_amortization_inputs)\n",
    "\n",
    "        # reparameterization trick to take a sample for W\n",
    "        eps = tf.random.normal(tf.shape(q_mu), dtype=gpflow.config.default_float())\n",
    "        W = q_mu + eps * q_sqrt\n",
    "        samples = tf.concat([F, W], -1)\n",
    "        mean = tf.concat([F, q_mu], -1)\n",
    "        cov = tf.concat([tf.zeros_like(F), q_sqrt ** 2], -1)\n",
    "        \n",
    "        #### HAHAHASDFDDADSF\n",
    "        ##### AGGHHH NOTICE THE SCALE ON THE KL\n",
    "        ##### ITS NOT 1!!!!!\n",
    "\n",
    "        # the prior regularization\n",
    "        p = p = tfp.distributions.Normal(loc=tf.zeros(1,dtype=gpflow.config.default_float()),\n",
    "                             scale=tf.ones(1,dtype=gpflow.config.default_float()))\n",
    "        q = tfp.distributions.Normal(loc=q_mu, scale=q_sqrt)\n",
    "\n",
    "        if is_sampled_local_regularizer:\n",
    "            # for the IW models, we need to return a log q/p for each sample W\n",
    "            kl = q.log_prob(W) - p.log_prob(W)\n",
    "        else:\n",
    "            # for the VI models, we want E_q log q/p, which is closed form for Gaussians\n",
    "            kl = tfp.distributions.kl_divergence(q, p)\n",
    "\n",
    "        return samples, mean, cov, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmortizedLatentVariableLayerTiled(gpflow.Module):\n",
    "    regularizer_type = RegularizerType.LOCAL\n",
    "    def __init__(self, latent_dim: int,\n",
    "                 sase_dim: int,\n",
    "                 encoder_dims: Optional[List[int]] = None):\n",
    "        super().__init__()\n",
    "        if encoder_dims is None:\n",
    "            encoder = Encoder(latent_dim, sase_dim, [50, 10, 10])\n",
    "        else:\n",
    "            encoder = Encoder(latent_dim, sase_dim, encoder_dims)\n",
    "        self.latent_dim = tf.convert_to_tensor([latent_dim], dtype=tf.int32)\n",
    "        self.sase_dim = tf.convert_to_tensor([sase_dim], dtype=tf.int32)\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def propagate(self, F: tf.Tensor,\n",
    "                  inference_amortization_inputs: Optional[tf.Tensor] = None,\n",
    "                  is_sampled_local_regularizer: bool = False,\n",
    "                  **kwargs) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "        if inference_amortization_inputs is None:\n",
    "            \"\"\"\n",
    "            If there isn't a SASE spec passed for the recognition model, this samples from the prior.\n",
    "            Optionally, q_mu and q_sqrt can be fed via a placeholder (e.g. for plotting purposes)\n",
    "            \"\"\"\n",
    "            batch_shape = tf.convert_to_tensor(tf.shape(F)[:-3], dtype=tf.int32) # ...\n",
    "            N = tf.convert_to_tensor([tf.shape(F)[-3]], dtype=tf.int32)\n",
    "            S = tf.convert_to_tensor([tf.shape(F)[-2]//self.sase_dim[0]], dtype=tf.int32)\n",
    "            shape = tf.concat([batch_shape, N, S, self.latent_dim], 0) # ...(N)L\n",
    "            q_mu = tf.zeros(shape, dtype=gpflow.config.default_float())\n",
    "            q_sqrt = tf.ones(shape, dtype=gpflow.config.default_float())\n",
    "        else:\n",
    "            q_mu, q_sqrt = self.encoder(inference_amortization_inputs)\n",
    "\n",
    "        # reparameterization trick to take a sample for W\n",
    "        eps = tf.random.normal(tf.shape(q_mu), dtype=gpflow.config.default_float())\n",
    "        W = q_mu + eps * q_sqrt\n",
    "        tile_vec = tf.concat([tf.convert_to_tensor([1], dtype=tf.int32),\n",
    "                              self.sase_dim, tf.convert_to_tensor([1], dtype=tf.int32)],0)\n",
    "        TW = tf.tile(W,tile_vec)\n",
    "        Tmu = tf.tile(q_mu, tile_vec)\n",
    "        Tsqrt = tf.tile(q_sqrt, tile_vec)\n",
    "        samples = tf.concat([F, TW], -1)\n",
    "        mean = tf.concat([F, Tmu], -1)\n",
    "        cov = tf.concat([tf.zeros_like(F), Tsqrt ** 2], -1)\n",
    "\n",
    "        # the prior regularization\n",
    "        p = p = tfp.distributions.Normal(loc=tf.zeros(1,dtype=gpflow.config.default_float()),\n",
    "                             scale=0.1*tf.ones(1,dtype=gpflow.config.default_float()))\n",
    "        q = tfp.distributions.Normal(loc=Tmu, scale=Tsqrt)\n",
    "\n",
    "        if is_sampled_local_regularizer:\n",
    "            # for the IW models, we need to return a log q/p for each sample W\n",
    "            kl = q.log_prob(TW) - p.log_prob(TW)\n",
    "        else:\n",
    "            # for the VI models, we want E_q log q/p, which is closed form for Gaussians\n",
    "            kl = tfp.distributions.kl_divergence(q, p)\n",
    "\n",
    "        return samples, mean, cov, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "@attr.s(auto_attribs=True)\n",
    "class GPLayer_Config(object):\n",
    "    ngps: int\n",
    "    ninducing: int\n",
    "        \n",
    "@attr.s(auto_attribs=True)\n",
    "class LatentLayer_Config(object):\n",
    "    latent_features: int\n",
    "    xy_dim: int\n",
    "        \n",
    "@attr.s(auto_attribs=True)\n",
    "class EmbeddingLatentLayer_Config(object):\n",
    "    latent_features: int\n",
    "    nwords: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
