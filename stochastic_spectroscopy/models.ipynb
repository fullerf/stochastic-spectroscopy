{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/dgp_iwvi_gpflow2/')\n",
    "import gpflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Tuple, Optional, List, Union\n",
    "import dgp_iwvi_gpflow2.layers as layers\n",
    "import attr\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegressionData = Tuple[tf.Tensor, tf.Tensor]\n",
    "AuxRegressionData = Tuple[tf.Tensor, tf.Tensor, tf.Tensor]\n",
    "InputData = tf.Tensor\n",
    "MeanVarKL = Tuple[tf.Tensor, tf.Tensor, tf.Tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLGP(gpflow.models.BayesianModel, gpflow.models.ExternalDataTrainingLossMixin):\n",
    "    \"\"\"\n",
    "    This is a \"Single Layer Gaussian Process\" model, it should be identical in behavior/performance to the\n",
    "    svgp model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer: layers.GPLayer,\n",
    "        likelihood: gpflow.likelihoods.Likelihood,\n",
    "        *,\n",
    "        num_data: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        - layer: an instance of GPLayer\n",
    "        - num_data is the total number of observations, defaults to X.shape[0]\n",
    "          (relevant when feeding in external minibatches)\n",
    "        \"\"\"\n",
    "        # init the super class, accept args\n",
    "        super().__init__()\n",
    "        self.num_data = num_data\n",
    "        self.layer = layer\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def maximum_log_likelihood_objective(self, data: RegressionData) -> tf.Tensor:\n",
    "        return self.elbo(data)\n",
    "\n",
    "    def elbo(self, data: RegressionData) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        This gives a variational bound (the evidence lower bound or ELBO) on\n",
    "        the log marginal likelihood of the model.\n",
    "        \"\"\"\n",
    "        X, Y = data\n",
    "        f_mean, f_var, kl = self.predict_f(X, full_cov=False)\n",
    "        var_exp = self.likelihood.variational_expectations(f_mean, f_var, Y)\n",
    "        if self.num_data is not None:\n",
    "            num_data = tf.cast(self.num_data, kl.dtype)\n",
    "            minibatch_size = tf.cast(tf.shape(X)[0], kl.dtype)\n",
    "            scale = num_data / minibatch_size\n",
    "        else:\n",
    "            scale = tf.cast(1.0, kl.dtype)\n",
    "        return tf.reduce_sum(var_exp) * scale - kl\n",
    "\n",
    "    def predict_f(self, Xnew: InputData, full_cov=False) -> MeanVarKL:\n",
    "        mu, var, kl = self.layer.components(\n",
    "            Xnew,\n",
    "            full_cov=full_cov,\n",
    "        )\n",
    "        return mu, var, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPMMData = Tuple[tf.Tensor, tf.Tensor]\n",
    "\n",
    "class SLGPMM(gpflow.models.BayesianModel, gpflow.models.ExternalDataTrainingLossMixin):\n",
    "    \"\"\"\n",
    "    This is a \"Single Layer Gaussian Process Mixture of Measurements\" model, wherein the\n",
    "    observed variable is linear mixed by a known mixing matrix W.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer: layers.GPLayer,\n",
    "        likelihood: gpflow.likelihoods.Likelihood,\n",
    "        reduction_axis_len: int,\n",
    "        *,\n",
    "        num_data: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        - layer: an instance of GPLayer\n",
    "        - num_data is the total number of observations, defaults to X.shape[0]\n",
    "          (relevant when feeding in external minibatches)\n",
    "        \"\"\"\n",
    "        # init the super class, accept args\n",
    "        super().__init__()\n",
    "        self.num_data = num_data\n",
    "        self.layer = layer\n",
    "        self.likelihood = likelihood\n",
    "        self.reduction_axis_len = int(reduction_axis_len)\n",
    "        reduction_axis = tf.convert_to_tensor(np.linspace(-np.pi,np.pi,self.reduction_axis_len)[:,None],\n",
    "                                                  gpflow.config.default_float())\n",
    "        self.raxis = gpflow.Parameter(reduction_axis, transform=None)\n",
    "        gpflow.utilities.set_trainable(self.raxis, False)\n",
    "\n",
    "    def maximum_log_likelihood_objective(self, data: GPMMData) -> tf.Tensor:\n",
    "        return self.elbo(data)\n",
    "\n",
    "    def elbo(self, data: GPMMData) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        This gives a variational bound (the evidence lower bound or ELBO) on\n",
    "        the log marginal likelihood of the model.\n",
    "        \"\"\"\n",
    "        W, Y = data\n",
    "        f_mean, f_var, kl = self.predict_f(self.raxis)  # R,1, R,R\n",
    "        f_mean_reduced = tf.squeeze(tf.einsum('nr,...rd->...n',W,f_mean))[:,None] # N -> N,1\n",
    "        f_var_reduced = tf.squeeze(tf.einsum('nr,...rq,nq->...n',W,f_var,W))[:,None] # N -> N,1\n",
    "        var_exp = self.likelihood.variational_expectations(f_mean_reduced, f_var_reduced, Y)\n",
    "        if self.num_data is not None:\n",
    "            num_data = tf.cast(self.num_data, kl.dtype)\n",
    "            minibatch_size = tf.cast(tf.shape(W)[0], kl.dtype)\n",
    "            scale = num_data / minibatch_size\n",
    "        else:\n",
    "            scale = tf.cast(1.0, kl.dtype)\n",
    "        return tf.reduce_sum(var_exp) * scale - kl\n",
    "\n",
    "    def predict_f(self, Xnew: tf.Tensor, full_cov=True) -> MeanVarKL:\n",
    "        mu, var, kl = self.layer.components(\n",
    "            Xnew,\n",
    "            full_cov=full_cov,\n",
    "        )\n",
    "        return mu, var, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LVNGP(gpflow.models.BayesianModel, gpflow.models.ExternalDataTrainingLossMixin):\n",
    "    \"\"\"\n",
    "    This is the basic model mentioned in Hugh Salembini's DGP IWVI paper, with N GP layers\n",
    "    beginning with a single amortized latent layer.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 likelihood: gpflow.likelihoods.Likelihood,\n",
    "                 latent_layer: layers.AmortizedLatentVariableLayer,\n",
    "                 gp_layers: List[layers.GPLayer],\n",
    "                 *,\n",
    "                 num_samples: int = 1,\n",
    "                 num_data: Optional[int] = None,\n",
    "                 local_kl_scale: Optional[float] = None,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.likelihood = likelihood\n",
    "        self.num_samples = num_samples\n",
    "        self.num_data = num_data\n",
    "        self.layers = [latent_layer] + gp_layers\n",
    "        if local_kl_scale is not None:\n",
    "            self.local_kl_scale = tf.convert_to_tensor(float(local_kl_scale), dtype=gpflow.config.default_float())\n",
    "        \n",
    "    def propagate(self, X: tf.Tensor,\n",
    "                  full_cov: bool = False,\n",
    "                  inference_amortization_inputs: Optional[tf.Tensor] = None,\n",
    "                  is_sampled_local_regularizer: bool = False):\n",
    "        samples, means, covs, kls, kl_types = [X, ], [], [], [], []\n",
    "\n",
    "        for i,layer in enumerate(self.layers):\n",
    "#             full_cov_local = False if i < len(self.layers)-1 else full_cov\n",
    "            full_cov_local = full_cov\n",
    "            sample, mean, cov, kl = layer.propagate(samples[-1],\n",
    "                                                    full_cov=full_cov_local,\n",
    "                                                    inference_amortization_inputs=inference_amortization_inputs,\n",
    "                                                    is_sampled_local_regularizer=is_sampled_local_regularizer)\n",
    "            samples.append(sample)\n",
    "            means.append(mean)\n",
    "            covs.append(cov)\n",
    "            kls.append(kl)\n",
    "            kl_types.append(layer.regularizer_type)\n",
    "\n",
    "        return samples[1:], means, covs, kls, kl_types\n",
    "        \n",
    "    def maximum_log_likelihood_objective(self, \n",
    "                                         data: AuxRegressionData) -> tf.Tensor:\n",
    "        return self.elbo(data)\n",
    "    \n",
    "    def elbo(self, \n",
    "             data: AuxRegressionData) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        This gives a variational bound (the evidence lower bound or ELBO) on\n",
    "        the log marginal likelihood of the model.\n",
    "        \"\"\"\n",
    "        X, Y, XY = data\n",
    "        X_tiled = tf.tile(X, [self.num_samples, 1])  # SN, Dx\n",
    "        Y_tiled = tf.tile(Y, [self.num_samples, 1])  # SN, Dy\n",
    "        XY_tiled = tf.tile(XY, [self.num_samples, 1])  # SN, Dy\n",
    "        samples, means, covs, kls, kl_types = self.propagate(X_tiled,\n",
    "                                                             full_cov=False,\n",
    "                                                             inference_amortization_inputs=XY_tiled,\n",
    "                                                             is_sampled_local_regularizer=False)\n",
    "        if self.local_kl_scale is not None:\n",
    "            local_kls = [kl*self.local_kl_scale for kl, t in zip(kls, kl_types) if t is layers.RegularizerType.LOCAL]\n",
    "        else:\n",
    "            local_kls = [kl for kl, t in zip(kls, kl_types) if t is layers.RegularizerType.LOCAL]\n",
    "        global_kls = [kl for kl, t in zip(kls, kl_types) if t is layers.RegularizerType.GLOBAL]\n",
    "        L_SN = self.likelihood.variational_expectations(means[-1], covs[-1], Y_tiled)  # SN\n",
    "       \n",
    "        # separate out repeated samples\n",
    "        shape_S_N = tf.concat([tf.convert_to_tensor([self.num_samples], dtype=tf.int32),\n",
    "                               tf.convert_to_tensor([tf.shape(X)[0]], dtype=tf.int32)],0)\n",
    "        L_S_N = tf.reshape(L_SN, shape_S_N)\n",
    "        \n",
    "        if len(local_kls) > 0:\n",
    "            local_kls_SN_D = tf.concat(local_kls, -1)  # SN, sum(W_dims)\n",
    "            local_kls_SN = tf.reduce_sum(local_kls_SN_D, -1)\n",
    "            local_kls_S_N = tf.reshape(local_kls_SN, shape_S_N)\n",
    "            L_S_N -= local_kls_S_N  # SN\n",
    "            \n",
    "        global_kl = tf.reduce_sum(global_kls)\n",
    "            \n",
    "        if self.num_data is not None:\n",
    "            num_data = tf.cast(self.num_data, global_kl.dtype)\n",
    "            minibatch_size = tf.cast(tf.shape(X)[0], global_kl.dtype)\n",
    "            scale = num_data / minibatch_size\n",
    "        else:\n",
    "            scale = tf.cast(1.0, global_kl.dtype)\n",
    "\n",
    "        # This line is replaced with tf.reduce_logsumexp in the IW case\n",
    "        logp = tf.reduce_mean(L_S_N, 0)\n",
    "\n",
    "        return tf.reduce_sum(logp) * scale - global_kl\n",
    "    \n",
    "    def predict_f_multisample(self, X, S):\n",
    "        X_tiled = tf.tile(X[None, :, :], [S, 1, 1])\n",
    "        _, means, covs, _, _ = self.propagate(X_tiled)\n",
    "        return means[-1], covs[-1]\n",
    "\n",
    "    def predict_y_samples(self, X, S):\n",
    "        X_tiled = tf.tile(X[None, :, :], [S, 1, 1])\n",
    "        _, means, covs, _, _ = self.propagate(X_tiled)\n",
    "        m, v = self.likelihood.predict_mean_and_var(means[-1], covs[-1])\n",
    "        z = tf.random.normal(tf.shape(means[-1]), dtype=X.dtype)\n",
    "        return m + z * v**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class LVNGP_IWVI(LVNGP):\n",
    "    def __init__(self,\n",
    "                 likelihood: gpflow.likelihoods.Likelihood,\n",
    "                 latent_layer: layers.AmortizedLatentVariableLayer,\n",
    "                 gp_layers: List[layers.GPLayer],\n",
    "                 *,\n",
    "                 num_samples: int = 1,\n",
    "                 num_data: Optional[int] = None):\n",
    "        super().__init__(likelihood, latent_layer, gp_layers, num_samples=num_samples, num_data=num_data)\n",
    "        \n",
    "    def elbo(self, \n",
    "             data: AuxRegressionData) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        This gives a variational bound (the evidence lower bound or ELBO) on\n",
    "        the log marginal likelihood of the model.\n",
    "        \"\"\"\n",
    "        X, Y, XY = data\n",
    "        X_tiled = tf.tile(X[:, None, :], [1, self.num_samples, 1])  # N, S, Dx\n",
    "        Y_tiled = tf.tile(Y[:, None, :], [1, self.num_samples, 1])  # N, S, Dy\n",
    "        XY_tiled = tf.tile(XY[:, None, :], [1, self.num_samples, 1])  # N, S, Dxy\n",
    "        samples, means, covs, kls, kl_types = self.propagate(X_tiled,\n",
    "                                                             full_cov=True,  # full_cov is over the S dim\n",
    "                                                             inference_amortization_inputs=XY_tiled,\n",
    "                                                             is_sampled_local_regularizer=True)\n",
    "\n",
    "        local_kls = [kl for kl, t in zip(kls, kl_types) if t is layers.RegularizerType.LOCAL]\n",
    "        global_kls = [kl for kl, t in zip(kls, kl_types) if t is layers.RegularizerType.GLOBAL]\n",
    "\n",
    "        # This could be made slightly more efficient by making the last layer full_cov=False,\n",
    "        # but this seems a small price to pay for cleaner code. NB this is only a SxS matrix, not\n",
    "        # an NxN matrix.\n",
    "        cov_diag = tf.transpose(tf.linalg.diag_part(covs[-1]), [0, 2, 1])  # N,Dy,S,S -> N,S,Dy\n",
    "        L_N_S = self.likelihood.variational_expectations(means[-1], cov_diag, Y_tiled)  # N, S\n",
    "        \n",
    "        if len(local_kls) > 0:\n",
    "            local_kls_N_S_D = tf.concat(local_kls, -1)  # N, S, sum(W_dims)\n",
    "            local_kls_N_S = tf.reduce_sum(local_kls_N_S_D, -1)\n",
    "            L_N_S -= local_kls_N_S  # N,S\n",
    "            \n",
    "        global_kl = tf.reduce_sum(global_kls)\n",
    "            \n",
    "        if self.num_data is not None:\n",
    "            num_data = tf.cast(self.num_data, global_kl.dtype)\n",
    "            minibatch_size = tf.cast(tf.shape(X)[0], global_kl.dtype)\n",
    "            scale = num_data / minibatch_size\n",
    "        else:\n",
    "            scale = tf.cast(1.0, global_kl.dtype)\n",
    "\n",
    "        # This line is replaced with tf.reduce_logsumexp in the IW case\n",
    "        logp = tf.reduce_logsumexp(L_N_S, -1) - np.log(self.num_samples)\n",
    "        return tf.reduce_sum(logp) * scale - global_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s(auto_attribs=True)\n",
    "class LVNGP_Config(object):\n",
    "    latent_layer: Union[layers.LatentLayer_Config, layers.EmbeddingLatentLayer_Config]\n",
    "    gplayers: List[layers.GPLayer_Config]\n",
    "    var_init: float\n",
    "    latent_samples: int\n",
    "    inference_type: str\n",
    "    local_kl_scale: Optional[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGP_GPMM(gpflow.models.BayesianModel, gpflow.models.ExternalDataTrainingLossMixin):\n",
    "    \"\"\"\n",
    "    Garden variety doubly stochastic DGP with GPMM likelihood\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 likelihood: gpflow.likelihoods.Likelihood,\n",
    "                 gp_layers: List[layers.GPLayer],\n",
    "                 reduction_axis_len: int,\n",
    "                 *,\n",
    "                 num_samples: int = 1,\n",
    "                 num_data: Optional[int] = None,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.likelihood = likelihood\n",
    "        self.num_samples = num_samples\n",
    "        self.num_data = num_data\n",
    "        self.reduction_axis_len = int(reduction_axis_len)\n",
    "        reduction_axis = tf.convert_to_tensor(np.linspace(-np.pi,np.pi,self.reduction_axis_len)[:,None],\n",
    "                                                  gpflow.config.default_float())\n",
    "        self.raxis = gpflow.Parameter(reduction_axis, transform=None)\n",
    "        gpflow.utilities.set_trainable(self.raxis, False)\n",
    "        self.layers = gp_layers\n",
    "        \n",
    "    def propagate(self, X: tf.Tensor,\n",
    "                  full_cov: bool = False):\n",
    "        samples, means, covs, kls, kl_types = [X, ], [], [], [], []\n",
    "\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            sample, mean, cov, kl = layer.propagate(samples[-1],\n",
    "                                                    full_cov=full_cov)\n",
    "            samples.append(sample)\n",
    "            means.append(mean)\n",
    "            covs.append(cov)\n",
    "            kls.append(kl)\n",
    "            kl_types.append(layer.regularizer_type)\n",
    "\n",
    "        return samples[1:], means, covs, kls, kl_types\n",
    "        \n",
    "    def maximum_log_likelihood_objective(self, data: AuxRegressionData) -> tf.Tensor:\n",
    "        return self.elbo(data)\n",
    "    \n",
    "    def elbo(self, \n",
    "             data: Tuple[tf.Tensor, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        This gives a variational bound (the evidence lower bound or ELBO) on\n",
    "        the log marginal likelihood of the model.\n",
    "        \"\"\"\n",
    "        W, Y = data\n",
    "        X_tiled = tf.tile(self.raxis[None, :, :], [self.num_samples, 1, 1])  # SRD\n",
    "        Y_tiled = tf.tile(Y[None, :, :], [self.num_samples, 1, 1])  # SND\n",
    "        samples, means, covs, kls, kl_types = self.propagate(X_tiled,\n",
    "                                                             full_cov=True)\n",
    "        global_kls = [kl for kl, t in zip(kls, kl_types) if t is layers.RegularizerType.GLOBAL]\n",
    "        f_mean = means[-1] # SRD\n",
    "        f_cov = covs[-1] # SDRR\n",
    "        f_mean_reduced =tf.einsum('nr,...rd->...nd',W,f_mean) # SRD -> SND\n",
    "        f_var_reduced = tf.einsum('nr,...drq,nq->...nd',W,f_cov,W) # SDRR -> SND\n",
    "        L_S_N = self.likelihood.variational_expectations(f_mean_reduced, f_var_reduced, Y_tiled)  # SND\n",
    "        global_kl = tf.reduce_sum(global_kls)\n",
    "            \n",
    "        if self.num_data is not None:\n",
    "            num_data = tf.cast(self.num_data, global_kl.dtype)\n",
    "            minibatch_size = tf.cast(tf.shape(W)[0], global_kl.dtype)\n",
    "            scale = num_data / minibatch_size\n",
    "        else:\n",
    "            scale = tf.cast(1.0, global_kl.dtype)\n",
    "\n",
    "        # This line is replaced with tf.reduce_logsumexp in the IW case\n",
    "        logp = tf.reduce_mean(L_S_N, 0)\n",
    "\n",
    "        return tf.reduce_sum(logp) * scale - global_kl\n",
    "    \n",
    "    def predict_f_multisample(self, X, S):\n",
    "        X_tiled = tf.tile(X[None, :, :], [S, 1, 1])\n",
    "        _, means, covs, _, _ = self.propagate(X_tiled)\n",
    "        return means[-1], covs[-1]\n",
    "\n",
    "    def predict_y_samples(self, X, S):\n",
    "        X_tiled = tf.tile(X[None, :, :], [S, 1, 1])\n",
    "        _, means, covs, _, _ = self.propagate(X_tiled)\n",
    "        m, v = self.likelihood.predict_mean_and_var(means[-1], covs[-1])\n",
    "        z = tf.random.normal(tf.shape(means[-1]), dtype=X.dtype)\n",
    "        return m + z * v**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s(auto_attribs=True)\n",
    "class DGP_GPMM_Config(object):\n",
    "    gplayers: List[layers.GPLayer_Config]\n",
    "    var_init: float\n",
    "    num_samples: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LVNDGP_GPMM_IWVI_MarginalApprox(gpflow.models.BayesianModel, gpflow.models.ExternalDataTrainingLossMixin):\n",
    "    \"\"\"\n",
    "    Garden variety doubly stochastic DGP with GPMM likelihood, latent inputs, and importance sampling. We\n",
    "    approximate the full GPMM distribution by marginalizing before carrying out reduction rather than after.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 likelihood: gpflow.likelihoods.Likelihood,\n",
    "                 latent_layer: layers.AmortizedSASELatentVariableLayer,\n",
    "                 gp_layers: List[layers.GPLayer],\n",
    "                 reduction_axis_len: int,\n",
    "                 *,\n",
    "                 num_samples: int = 1,\n",
    "                 num_data: Optional[int] = None,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.likelihood = likelihood\n",
    "        self.num_samples = num_samples\n",
    "        self.num_data = num_data\n",
    "        self.reduction_axis_len = int(reduction_axis_len)\n",
    "        reduction_axis = tf.convert_to_tensor(np.linspace(-np.pi,np.pi,self.reduction_axis_len)[:,None],\n",
    "                                                  gpflow.config.default_float())\n",
    "        self.raxis = gpflow.Parameter(reduction_axis, transform=None)\n",
    "        gpflow.utilities.set_trainable(self.raxis, False)\n",
    "        self.latent_layer = latent_layer\n",
    "        self.layers = gp_layers\n",
    "        \n",
    "    def propagate(self, X: tf.Tensor,\n",
    "                  full_cov: bool = False,\n",
    "                 is_sampled_local_regularizer: bool = False):\n",
    "        samples, means, covs, kls, kl_types = [X, ], [], [], [], []\n",
    "\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            sample, mean, cov, kl = layer.propagate(samples[-1],\n",
    "                                                    full_cov=full_cov,\n",
    "                                                   is_sampled_local_regularizer=is_sampled_local_regularizer)\n",
    "            samples.append(sample)\n",
    "            means.append(mean)\n",
    "            covs.append(cov)\n",
    "            kls.append(kl)\n",
    "            kl_types.append(layer.regularizer_type)\n",
    "\n",
    "        return samples[1:], means, covs, kls, kl_types\n",
    "        \n",
    "    def maximum_log_likelihood_objective(self, data: AuxRegressionData) -> tf.Tensor:\n",
    "        return self.elbo(data)\n",
    "    \n",
    "    def elbo(self, \n",
    "             data: Tuple[tf.Tensor, tf.Tensor, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        This gives a variational bound (the evidence lower bound or ELBO) on\n",
    "        the log marginal likelihood of the model.\n",
    "        \"\"\"\n",
    "        W, Y, Wstd = data\n",
    "        N = tf.shape(W)[0] # expecting NR shape\n",
    "        R = tf.shape(self.raxis)[0] # expecting R1 shape\n",
    "        X_tiled = tf.tile(self.raxis[None, :, :], [self.num_samples, N, 1])  # S, N*R, Din\n",
    "        W_tiled = tf.tile(Wstd[None, :, :], [self.num_samples, 1, 1]) # S, N, R\n",
    "        Y_tiled = tf.tile(Y[:, None, :], [1, self.num_samples, 1]) # N, S, Dout\n",
    "        # get the latent sample\n",
    "        lsamp, lmean, lcov, lkl = self.latent_layer.propagate(X_tiled, inference_amortization_inputs=W_tiled,\n",
    "                                                             is_sampled_local_regularizer=True)\n",
    "        \n",
    "        #lsamp has shape ..., S, N*R, Din + Dlatent. Want to reshape to ..., N*R, S, Din + Dlatent\n",
    "        lsamp_reshaped = tf.einsum('...ijk->...jik',lsamp)  # N*R, S, Din + Dlatent\n",
    "        lmean_reshaped = tf.einsum('...ijk->...jik',lmean)  # N*R, S, Din + Dlatent\n",
    "        lcov_reshaped = tf.einsum('...ijk->...jik',lcov)  # N*R, S, Din + Dlatent\n",
    "        lkl_reshaped = tf.einsum('...ijk->...jik',lkl)  # N*R, S, Dlatent\n",
    "        \n",
    "        samples, means, covs, kls, kl_types = self.propagate(lsamp_reshaped,\n",
    "                                                             full_cov=True,\n",
    "                                                            is_sampled_local_regularizer=False)\n",
    "        samples = [lsamp_reshaped, *samples]\n",
    "        means = [lmean_reshaped, *means]\n",
    "        lcov_reshaped = [lcov_reshaped, *covs]\n",
    "        kls = [lkl_reshaped, *kls]\n",
    "        kl_types = [self.latent_layer.regularizer_type, *kl_types]\n",
    "        \n",
    "        global_kls = [kl for kl, t in zip(kls, kl_types) if t is layers.RegularizerType.GLOBAL]\n",
    "        local_kls = [kl for kl, t in zip(kls, kl_types) if t is layers.RegularizerType.LOCAL]\n",
    "        \n",
    "        f_mean = means[-1]  # N*R, S, Dout\n",
    "        Dout = tf.shape(f_mean)[-1]\n",
    "        f_var = tf.linalg.diag_part(covs[-1])  # N*R, Dout, S, S -> N*R, Dout, S\n",
    "        \n",
    "        \n",
    "        f_mean_reshaped = tf.reshape(f_mean, [N, R, self.num_samples, Dout])  # N, R, S, Dout\n",
    "        f_var_reshaped = tf.reshape(f_var, [N, R, Dout, self.num_samples])  # N, R, Dout, S\n",
    "        f_mean_reduced =tf.einsum('nr,nrsd->nsd',W,f_mean_reshaped) # N, R, S, Dout -> N, S, Dout\n",
    "        f_var_reduced = tf.einsum('nr,nrds->nsd',W*W,f_var_reshaped)  # N, R, Dout, S  -> N, S, Dout\n",
    "        #variational_expectations reduces over -1 dim, leaving only S, N\n",
    "        \n",
    "        L_N_S = self.likelihood.variational_expectations(f_mean_reduced, f_var_reduced, Y_tiled)\n",
    "        \n",
    "        if len(local_kls) > 0:\n",
    "            local_kls_NR_S_D = tf.concat(local_kls, -1)  # N*R, S, sum(Dlatents)\n",
    "            Dl = tf.shape(local_kls_NR_S_D)[-1]\n",
    "            local_kls_N_R_S_D = tf.reshape(local_kls_NR_S_D, [N, R, self.num_samples, Dl])\n",
    "            local_kls_N_R_S = tf.reduce_sum(local_kls_N_R_S_D, -1)\n",
    "            local_kls_N_S = tf.reduce_sum(local_kls_N_R_S,-2)\n",
    "            L_N_S -= local_kls_N_S  # N,S\n",
    "        \n",
    "        global_kl = tf.reduce_sum(global_kls) # a scalar\n",
    "            \n",
    "        if self.num_data is not None:\n",
    "            num_data = tf.cast(self.num_data, global_kl.dtype)\n",
    "            minibatch_size = tf.cast(tf.shape(W)[0], global_kl.dtype)\n",
    "            scale = num_data / minibatch_size\n",
    "        else:\n",
    "            scale = tf.cast(1.0, global_kl.dtype)\n",
    "\n",
    "        # This line is replaced with tf.reduce_logsumexp in the IW case\n",
    "        logp = tf.reduce_logsumexp(L_N_S, -1)\n",
    "\n",
    "        return tf.reduce_sum(logp) * scale - global_kl\n",
    "    \n",
    "    def predict_f_multisample(self, X, S):\n",
    "        X_tiled = tf.tile(X[None, :, :], [S, 1, 1])\n",
    "        _, means, covs, _, _ = self.propagate(X_tiled)\n",
    "        return means[-1], covs[-1]\n",
    "\n",
    "    def predict_y_samples(self, X, S):\n",
    "        X_tiled = tf.tile(X[None, :, :], [S, 1, 1])\n",
    "        _, means, covs, _, _ = self.propagate(X_tiled)\n",
    "        m, v = self.likelihood.predict_mean_and_var(means[-1], covs[-1])\n",
    "        z = tf.random.normal(tf.shape(means[-1]), dtype=X.dtype)\n",
    "        return m + z * v**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGPMM_IWVI_MITM(gpflow.models.BayesianModel, gpflow.models.ExternalDataTrainingLossMixin):\n",
    "    \"\"\"\n",
    "    it's a DGP -> GPMM reduction -> DGP IWVI, so we have layers for the spectrum: spectrum_layers\n",
    "    (these get reduced by sase), then we enter the reduced layers.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 likelihood: gpflow.likelihoods.Likelihood,\n",
    "                 spectrum_layers: List[layers.GPLayer],\n",
    "                 reduced_layers: List[Union[layers.GPLayer, layers.AmortizedSASELatentVariableLayer]],\n",
    "                 reduction_axis_len: int,\n",
    "                 *,\n",
    "                 num_samples: int = 1,\n",
    "                 num_data: Optional[int] = None,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.likelihood = likelihood\n",
    "        self.num_samples = num_samples\n",
    "        self.num_data = num_data\n",
    "        self.reduction_axis_len = int(reduction_axis_len)\n",
    "        reduction_axis = tf.convert_to_tensor(np.linspace(-np.pi,np.pi,self.reduction_axis_len)[:,None],\n",
    "                                                  gpflow.config.default_float())\n",
    "        self.raxis = gpflow.Parameter(reduction_axis, transform=None)\n",
    "        gpflow.utilities.set_trainable(self.raxis, False)\n",
    "        self.latent_layer = latent_layer\n",
    "        self.spectrum_layers = spectrum_layers\n",
    "        self.reduced_layers = reduced_layers\n",
    "        \n",
    "    def propagate(self, X: tf.Tensor, \n",
    "                  W: Optional[tf.Tensor] = None,\n",
    "                  full_cov: bool = False,\n",
    "                  inference_amortization_inputs: Optional[tf.Tensor] = None,\n",
    "                  is_sampled_local_regularizer: bool = False):\n",
    "        samples, means, covs, kls, kl_types = [X, ], [], [], [], []\n",
    "\n",
    "        for layer in self.spectrum_layers:\n",
    "            sample, mean, cov, kl = layer.propagate(samples[-1],\n",
    "                                                    full_cov=True,\n",
    "                                                    is_sampled_local_regularizer=is_sampled_local_regularizer,\n",
    "                                                    inference_amortization_inputs=inference_amortization_inputs)\n",
    "            samples.append(sample)\n",
    "            means.append(mean)\n",
    "            covs.append(cov)\n",
    "            kls.append(kl)\n",
    "            kl_types.append(layer.regularizer_type)\n",
    "            \n",
    "        # reduce the latent space\n",
    "        if W is None:\n",
    "            # this case used for prediction of spectra\n",
    "            N = tf.shape(X)[-2]\n",
    "            W = tf.eye(N,N, dtype=gpflow.config.default_float())\n",
    "        \n",
    "        # we assume that W is always NxR\n",
    "        # below we produce the reduced mean and marginal variance\n",
    "        rmean = tf.einsum('nr,...rd->...nd',W,means[-1])\n",
    "        rvar = tf.einsum('nr,...drq,nq->...nd')\n",
    "        \n",
    "        # sample from the marginalized mean/var, producing a single sample\n",
    "        eps = tf.random.normal(tf.shape(rmean), dtype=gpflow.config.default_float())\n",
    "        rsample = rmean + eps * tf.sqrt(rvar)\n",
    "        \n",
    "        # now we tile the latent space to prepare for importance sampling\n",
    "        bdims = tf.shape(rsample)[:-2]\n",
    "        rsample_tiled = tf.tile(rsample[...,None,:], bdims*[1] + [self.num_samples, 1])\n",
    "        \n",
    "        means.append(rmean)\n",
    "        covs.append(rcov)\n",
    "        samples.append(rsample)\n",
    "        \n",
    "        for layer in self.reduced_layers:\n",
    "            sample, mean, cov, kl = layer.propagate(samples[-1],\n",
    "                                                    full_cov=True,\n",
    "                                                    is_sampled_local_regularizer=is_sampled_local_regularizer,\n",
    "                                                    inference_amortization_inputs=inference_amortization_inputs)\n",
    "            samples.append(sample)\n",
    "            means.append(mean)\n",
    "            covs.append(cov)\n",
    "            kls.append(kl)\n",
    "            kl_types.append(layer.regularizer_type)\n",
    "        \n",
    "\n",
    "        return samples[1:], means, covs, kls, kl_types\n",
    "        \n",
    "    def maximum_log_likelihood_objective(self, data: AuxRegressionData) -> tf.Tensor:\n",
    "        return self.elbo(data)\n",
    "    \n",
    "    def elbo(self, \n",
    "             data: Tuple[tf.Tensor, tf.Tensor, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        This gives a variational bound (the evidence lower bound or ELBO) on\n",
    "        the log marginal likelihood of the model.\n",
    "        \"\"\"\n",
    "        W, Y, Wstd = data\n",
    "        Wstd_tiled = tf.tile(Wstd[:, None, :], [1, self.num_samples, 1]) # N, S, R\n",
    "        Y_tiled = tf.tile(Y[:, None, :], [1, self.num_samples, 1]) # N, S, Dout\n",
    "        # get the latent sample\n",
    "        lsamp, lmean, lcov, lkl = self.latent_layer.propagate(self.raxis,\n",
    "                                                             inference_amortization_inputs=Wstd,\n",
    "                                                             is_sampled_local_regularizer=True)\n",
    "        \n",
    "        global_kls = [kl for kl, t in zip(kls, kl_types) if t is layers.RegularizerType.GLOBAL]\n",
    "        local_kls = [kl for kl, t in zip(kls, kl_types) if t is layers.RegularizerType.LOCAL]\n",
    "        \n",
    "        f_mean = means[-1]  # N, S, Dout\n",
    "        f_var = tf.transpose(tf.linalg.diag_part(covs[-1]),[0,2,1])  # N, Dout, S, S -> N, Dout, S -> N, S, Dout\n",
    "        L_N_S = self.likelihood.variational_expectations(f_mean, f_var, Y_tiled)\n",
    "        \n",
    "        if len(local_kls) > 0:\n",
    "            local_kls_N_S_D = tf.concat(local_kls, -1)  # N, S, sum(Dlatents)\n",
    "            local_kls_N_S = tf.reduce_sum(local_kls_N_S_D, -1)\n",
    "            L_N_S -= local_kls_N_S  # N,S\n",
    "        \n",
    "        global_kl = tf.reduce_sum(global_kls) # a scalar\n",
    "            \n",
    "        if self.num_data is not None:\n",
    "            num_data = tf.cast(self.num_data, global_kl.dtype)\n",
    "            minibatch_size = tf.cast(tf.shape(W)[0], global_kl.dtype)\n",
    "            scale = num_data / minibatch_size\n",
    "        else:\n",
    "            scale = tf.cast(1.0, global_kl.dtype)\n",
    "\n",
    "        # This line is replaced with tf.reduce_logsumexp in the IW case\n",
    "        logp = tf.reduce_logsumexp(L_N_S, -1)\n",
    "\n",
    "        return tf.reduce_sum(logp) * scale - global_kl\n",
    "    \n",
    "    def predict_f_multisample(self, X, S):\n",
    "        X_tiled = tf.tile(X[None, :, :], [S, 1, 1])\n",
    "        _, means, covs, _, _ = self.propagate(X_tiled)\n",
    "        return means[-1], covs[-1]\n",
    "\n",
    "    def predict_y_samples(self, X, S):\n",
    "        X_tiled = tf.tile(X[None, :, :], [S, 1, 1])\n",
    "        _, means, covs, _, _ = self.propagate(X_tiled)\n",
    "        m, v = self.likelihood.predict_mean_and_var(means[-1], covs[-1])\n",
    "        z = tf.random.normal(tf.shape(means[-1]), dtype=X.dtype)\n",
    "        return m + z * v**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LVNGP_GPMM_IWVI_Tiled(gpflow.models.BayesianModel, gpflow.models.ExternalDataTrainingLossMixin):\n",
    "    \"\"\"\n",
    "    LVNGP with GPMM likelihood. We prepare a latent input based on SASE spectrum and tile that input over\n",
    "    the reduction axis. This is then fed to the GP that produces a latent spectrum, which will be reduced\n",
    "    by the standard GPMM approach and compared to the output. We refer to latent inputs as being the result\n",
    "    of a function w, which behaves like:\n",
    "    \n",
    "    w(N x R) -> N x Dlatent === l\n",
    "    \n",
    "    l is combined with the energy axis with something like a Kronecker product, where Dlatent and Dinput are\n",
    "    treated as single elements and unpacked via concatenation after the kronecker product is \n",
    "    carried out. This is implemented by tiling and concatenation.\n",
    "    \n",
    "    e_axis = R x Din\n",
    "    X = pseudo_kron(l,e_axis) = N x R x Din + Dlatent\n",
    "      = tf.concatenate([tf.tile( l[:,None,:], [1,R,1]),\n",
    "                        tf.tile( e_axis[None,:,:], [N,1,1])], -1)\n",
    "                        \n",
    "    To ease implementation, we approximate tiling of the amortized output by instead tiling input to \n",
    "    the amortization network, so:\n",
    "    tf.tile( l[:,None,:], [1,R,1]) -> w(tf.tile(W[:,None,:], [1,R,1]))\n",
    "    \n",
    "    \n",
    "    feeding this into the gp gives:\n",
    "    \n",
    "    F = f(X) = N x R x Dout\n",
    "    \n",
    "    To include multi-sampling, we must have full covariance over R and S, so we do:\n",
    "    \n",
    "    l = w(tf.tile(W[:,None,:], [1,S*R,1])) = N x SR x Dlatent\n",
    "    e_axis = R x Din\n",
    "    X = pseudo_kron(l,e_axis) = N x SR x Din + Dlatent\n",
    "      = tf.concatenate([tf.tile( l, [1,R,1]),\n",
    "                        tf.tile( e_axis[None,:,:], [N,S,1])], -1)\n",
    "    \n",
    "    F = f(X) = N x SR x Dout\n",
    "    \n",
    "    To carry out the reduction, must reshape the mean/cov to isolate R:\n",
    "    \n",
    "    F_mean_reshaped = tf.reshape(*, [N,S,R,Dout])\n",
    "    F_cov_reshaped = tf.reshape(*, [N,Dout,S,R,S,R])\n",
    "    \n",
    "    \n",
    "    F_mean_reduced = tf.einsum('nr,nsrd->nsd',W,F_mean_reshaped)\n",
    "    F_cov_reduced = tf.einsum('nr,ndsrsq,nq->nsd'W,F_cov_reshaped,W)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 likelihood: gpflow.likelihoods.Likelihood,\n",
    "                 layers: List[layers.GPLayer],\n",
    "                 reduction_axis_len: int,\n",
    "                 *,\n",
    "                 num_samples: int = 1,\n",
    "                 num_data: Optional[int] = None,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.likelihood = likelihood\n",
    "        self.num_samples = num_samples\n",
    "        self.num_data = num_data\n",
    "        self.reduction_axis_len = int(reduction_axis_len)\n",
    "        reduction_axis = tf.convert_to_tensor(np.linspace(-np.pi,np.pi,self.reduction_axis_len)[:,None],\n",
    "                                                  gpflow.config.default_float())\n",
    "        self.raxis = gpflow.Parameter(reduction_axis, transform=None)\n",
    "        gpflow.utilities.set_trainable(self.raxis, False)\n",
    "        self.layers = layers\n",
    "        \n",
    "    def propagate(self, X: tf.Tensor,\n",
    "                  full_cov: bool = False,\n",
    "                  inference_amortization_inputs: Optional[tf.Tensor] = None,\n",
    "                  is_sampled_local_regularizer: bool = False):\n",
    "        samples, means, covs, kls, kl_types = [X, ], [], [], [], []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            full_cov_local = full_cov\n",
    "            sample, mean, cov, kl = layer.propagate(samples[-1],\n",
    "                                                    full_cov=full_cov_local,\n",
    "                                                    inference_amortization_inputs=inference_amortization_inputs,\n",
    "                                                    is_sampled_local_regularizer=is_sampled_local_regularizer)\n",
    "            samples.append(sample)\n",
    "            means.append(mean)\n",
    "            covs.append(cov)\n",
    "            kls.append(kl)\n",
    "            kl_types.append(layer.regularizer_type)\n",
    "\n",
    "        return samples[1:], means, covs, kls, kl_types\n",
    "        \n",
    "    def maximum_log_likelihood_objective(self, data: AuxRegressionData) -> tf.Tensor:\n",
    "        return self.elbo(data)\n",
    "    \n",
    "    def elbo(self, \n",
    "             data: Tuple[tf.Tensor, tf.Tensor, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        We accept a triplet of inputs: SASE, Fluorescence, and Standardized SASE\n",
    "        \n",
    "        the last is used in the amortized latent inference network\n",
    "        \"\"\"\n",
    "        W, Y, Wstd = data\n",
    "        N = tf.shape(W)[0]\n",
    "        R = tf.shape(W)[1]\n",
    "        Dout = tf.shape(Y)[-1]\n",
    "        S = self.num_samples\n",
    "        Wstd_tiled = tf.tile(Wstd[:,None,:], [1,S*R,1])  # N, SR, R\n",
    "        Y_tiled = tf.tile(Y[:,None,:], [1,S,1])  # N,S,Dout\n",
    "        X = self.raxis\n",
    "        X_tiled = tf.tile(X[None,:,:], [N,S,1])  # N,SR,Din\n",
    "        \n",
    "        # get the latent sample\n",
    "        samples, means, covs, kls, kl_types = self.propagate(X_tiled,\n",
    "                                                             full_cov=True,  # full_cov is over the S dim\n",
    "                                                             inference_amortization_inputs=Wstd_tiled,\n",
    "                                                             is_sampled_local_regularizer=True)\n",
    "\n",
    "        local_kls = [kl for kl, t in zip(kls, kl_types) if t is layers.RegularizerType.LOCAL]\n",
    "        global_kls = [kl for kl, t in zip(kls, kl_types) if t is layers.RegularizerType.GLOBAL]\n",
    "        f_mean = tf.reshape(means[-1],[N,S,R,Dout])  # N, SR, Dout -> N, S, R, Dout\n",
    "        f_cov = tf.reshape(covs[-1],[N,Dout,S,R,S,R])  # N, Dout, SR, SR - > N, Dout, S, R, S, R\n",
    "        f_mean_reduced = tf.einsum('nr,nsrd->nsd',W,f_mean)\n",
    "        f_var_reduced = tf.einsum('nr,ndsrsq,nq->nsd',W,f_cov,W)\n",
    "        \n",
    "        \n",
    "        L_N_S = self.likelihood.variational_expectations(f_mean_reduced, f_var_reduced, Y_tiled)\n",
    "        \n",
    "        if len(local_kls) > 0:\n",
    "            local_kls_N_SR_D = tf.concat(local_kls, -1)  # N, SR, sum(Dlatents)\n",
    "            Dlatent = tf.shape(local_kls_N_SR_D)[-1]\n",
    "            local_kls_N_S_R_D = tf.reshape(local_kls_N_SR_D, [N, S, R, Dlatent])\n",
    "            local_kls_N_S = tf.reduce_sum(local_kls_N_S_R_D, [-1,-2])\n",
    "            L_N_S -= local_kls_N_S  # N,S\n",
    "        \n",
    "        global_kl = tf.reduce_sum(global_kls) # a scalar\n",
    "            \n",
    "        if self.num_data is not None:\n",
    "            num_data = tf.cast(self.num_data, global_kl.dtype)\n",
    "            minibatch_size = tf.cast(tf.shape(W)[0], global_kl.dtype)\n",
    "            scale = num_data / minibatch_size\n",
    "        else:\n",
    "            scale = tf.cast(1.0, global_kl.dtype)\n",
    "\n",
    "        # This line is replaced with tf.reduce_logsumexp in the IW case\n",
    "        logp = tf.reduce_logsumexp(L_N_S, -1)\n",
    "\n",
    "        return tf.reduce_sum(logp) * scale - global_kl\n",
    "    \n",
    "    def predict_f_multisample(self, X, S):\n",
    "        X_tiled = tf.tile(X[None, :, :], [S, 1, 1])\n",
    "        _, means, covs, _, _ = self.propagate(X_tiled)\n",
    "        return means[-1], covs[-1]\n",
    "\n",
    "    def predict_y_samples(self, X, S):\n",
    "        X_tiled = tf.tile(X[None, :, :], [S, 1, 1])\n",
    "        _, means, covs, _, _ = self.propagate(X_tiled)\n",
    "        m, v = self.likelihood.predict_mean_and_var(means[-1], covs[-1])\n",
    "        z = tf.random.normal(tf.shape(means[-1]), dtype=X.dtype)\n",
    "        return m + z * v**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LVNGP_GPMM_IWVI_Tiled2(gpflow.models.BayesianModel, gpflow.models.ExternalDataTrainingLossMixin):\n",
    "    \"\"\"\n",
    "    LVNGP with GPMM likelihood. We prepare a latent input based on SASE spectrum and tile that input over\n",
    "    the reduction axis. This is then fed to the GP that produces a latent spectrum, which will be reduced\n",
    "    by the standard GPMM approach and compared to the output. We refer to latent inputs as being the result\n",
    "    of a function w, which behaves like:\n",
    "    \n",
    "    w(N x R) -> N x Dlatent === l\n",
    "    \n",
    "    l is combined with the energy axis with something like a Kronecker product, where Dlatent and Dinput are\n",
    "    treated as single elements and unpacked via concatenation after the kronecker product is \n",
    "    carried out. This is implemented by tiling and concatenation.\n",
    "    \n",
    "    e_axis = R x Din\n",
    "    X = pseudo_kron(l,e_axis) = N x R x Din + Dlatent\n",
    "      = tf.concatenate([tf.tile( l[:,None,:], [1,R,1]),\n",
    "                        tf.tile( e_axis[None,:,:], [N,1,1])], -1)\n",
    "                        \n",
    "    To ease implementation, we approximate tiling of the amortized output by instead tiling input to \n",
    "    the amortization network, so:\n",
    "    tf.tile( l[:,None,:], [1,R,1]) -> w(tf.tile(W[:,None,:], [1,R,1]))\n",
    "    \n",
    "    \n",
    "    feeding this into the gp gives:\n",
    "    \n",
    "    F = f(X) = N x R x Dout\n",
    "    \n",
    "    To include multi-sampling, we must have full covariance over R and S, so we do:\n",
    "    \n",
    "    l = w(tf.tile(W[:,None,:], [1,S*R,1])) = N x SR x Dlatent\n",
    "    e_axis = R x Din\n",
    "    X = pseudo_kron(l,e_axis) = N x SR x Din + Dlatent\n",
    "      = tf.concatenate([tf.tile( l, [1,R,1]),\n",
    "                        tf.tile( e_axis[None,:,:], [N,S,1])], -1)\n",
    "    \n",
    "    F = f(X) = N x SR x Dout\n",
    "    \n",
    "    To carry out the reduction, must reshape the mean/cov to isolate R:\n",
    "    \n",
    "    F_mean_reshaped = tf.reshape(*, [N,S,R,Dout])\n",
    "    F_cov_reshaped = tf.reshape(*, [N,Dout,S,R,S,R])\n",
    "    \n",
    "    \n",
    "    F_mean_reduced = tf.einsum('nr,nsrd->nsd',W,F_mean_reshaped)\n",
    "    F_cov_reduced = tf.einsum('nr,ndsrsq,nq->nsd'W,F_cov_reshaped,W)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 likelihood: gpflow.likelihoods.Likelihood,\n",
    "                 layers: List[layers.GPLayer],\n",
    "                 reduction_axis_len: int,\n",
    "                 *,\n",
    "                 num_samples: int = 1,\n",
    "                 num_data: Optional[int] = None,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.likelihood = likelihood\n",
    "        self.num_samples = num_samples\n",
    "        self.num_data = num_data\n",
    "        self.reduction_axis_len = int(reduction_axis_len)\n",
    "        reduction_axis = tf.convert_to_tensor(np.linspace(-np.pi,np.pi,self.reduction_axis_len)[:,None],\n",
    "                                                  gpflow.config.default_float())\n",
    "        self.raxis = gpflow.Parameter(reduction_axis, transform=None)\n",
    "        gpflow.utilities.set_trainable(self.raxis, False)\n",
    "        self.layers = layers\n",
    "        \n",
    "    def propagate(self, X: tf.Tensor,\n",
    "                  full_cov: bool = False,\n",
    "                  inference_amortization_inputs: Optional[tf.Tensor] = None,\n",
    "                  is_sampled_local_regularizer: bool = False):\n",
    "        samples, means, covs, kls, kl_types = [X, ], [], [], [], []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            full_cov_local = full_cov\n",
    "            sample, mean, cov, kl = layer.propagate(samples[-1],\n",
    "                                                    full_cov=full_cov_local,\n",
    "                                                    inference_amortization_inputs=inference_amortization_inputs,\n",
    "                                                    is_sampled_local_regularizer=is_sampled_local_regularizer)\n",
    "            samples.append(sample)\n",
    "            means.append(mean)\n",
    "            covs.append(cov)\n",
    "            kls.append(kl)\n",
    "            kl_types.append(layer.regularizer_type)\n",
    "\n",
    "        return samples[1:], means, covs, kls, kl_types\n",
    "        \n",
    "    def maximum_log_likelihood_objective(self, data: AuxRegressionData) -> tf.Tensor:\n",
    "        return self.elbo(data)\n",
    "    \n",
    "    def elbo(self, \n",
    "             data: Tuple[tf.Tensor, tf.Tensor, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        We accept a triplet of inputs: SASE, Fluorescence, and Standardized SASE\n",
    "        \n",
    "        the last is used in the amortized latent inference network\n",
    "        \"\"\"\n",
    "        W, Y, Wstd = data\n",
    "        N = tf.shape(W)[0]\n",
    "        R = tf.shape(W)[1]\n",
    "        Dout = tf.shape(Y)[-1]\n",
    "        S = self.num_samples\n",
    "        Wstd_tiled = tf.tile(Wstd[:,None,:], [1,S,1])  # N, S, R\n",
    "        Y_tiled = tf.tile(Y[:,None,:], [1,S,1])  # N,S,Dout\n",
    "        X = self.raxis\n",
    "        X_tiled = tf.tile(X[None,:,:], [N,S,1])  # N,SR,Din\n",
    "        \n",
    "        # get the latent sample\n",
    "        samples, means, covs, kls, kl_types = self.propagate(X_tiled,\n",
    "                                                             full_cov=True,  # full_cov is over the S dim\n",
    "                                                             inference_amortization_inputs=Wstd_tiled,\n",
    "                                                             is_sampled_local_regularizer=True)\n",
    "\n",
    "        local_kls = [kl for kl, t in zip(kls, kl_types) if t is layers.RegularizerType.LOCAL]\n",
    "        global_kls = [kl for kl, t in zip(kls, kl_types) if t is layers.RegularizerType.GLOBAL]\n",
    "        f_mean = tf.reshape(means[-1],[N,S,R,Dout])  # N, SR, Dout -> N, S, R, Dout\n",
    "        f_cov = tf.reshape(covs[-1],[N,Dout,S,R,S,R])  # N, Dout, SR, SR - > N, Dout, S, R, S, R\n",
    "        f_mean_reduced = tf.einsum('nr,nsrd->nsd',W,f_mean)\n",
    "        f_var_reduced = tf.einsum('nr,ndsrsq,nq->nsd',W,f_cov,W)\n",
    "        \n",
    "        \n",
    "        L_N_S = self.likelihood.variational_expectations(f_mean_reduced, f_var_reduced, Y_tiled)\n",
    "        \n",
    "        if len(local_kls) > 0:\n",
    "            local_kls_N_SR_D = tf.concat(local_kls, -1)  # N, SR, sum(Dlatents)\n",
    "            Dlatent = tf.shape(local_kls_N_SR_D)[-1]\n",
    "            local_kls_N_S_R_D = tf.reshape(local_kls_N_SR_D, [N, S, R, Dlatent])\n",
    "            local_kls_N_S = tf.reduce_sum(local_kls_N_S_R_D, [-1,-2])\n",
    "            L_N_S -= local_kls_N_S  # N,S\n",
    "        \n",
    "        global_kl = tf.reduce_sum(global_kls) # a scalar\n",
    "            \n",
    "        if self.num_data is not None:\n",
    "            num_data = tf.cast(self.num_data, global_kl.dtype)\n",
    "            minibatch_size = tf.cast(tf.shape(W)[0], global_kl.dtype)\n",
    "            scale = num_data / minibatch_size\n",
    "        else:\n",
    "            scale = tf.cast(1.0, global_kl.dtype)\n",
    "\n",
    "        # This line is replaced with tf.reduce_logsumexp in the IW case\n",
    "        logp = tf.reduce_logsumexp(L_N_S, -1)\n",
    "\n",
    "        return tf.reduce_sum(logp) * scale - global_kl\n",
    "    \n",
    "    def predict_f_multisample(self, X, S):\n",
    "        X_tiled = tf.tile(X[None, :, :], [S, 1, 1])\n",
    "        _, means, covs, _, _ = self.propagate(X_tiled)\n",
    "        return means[-1], covs[-1]\n",
    "\n",
    "    def predict_y_samples(self, X, S):\n",
    "        X_tiled = tf.tile(X[None, :, :], [S, 1, 1])\n",
    "        _, means, covs, _, _ = self.propagate(X_tiled)\n",
    "        m, v = self.likelihood.predict_mean_and_var(means[-1], covs[-1])\n",
    "        z = tf.random.normal(tf.shape(means[-1]), dtype=X.dtype)\n",
    "        return m + z * v**0.5"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
